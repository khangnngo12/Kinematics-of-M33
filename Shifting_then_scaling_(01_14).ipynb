{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Importing and Defining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRATING = 600ZD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np \n",
    "from astropy.io import fits \n",
    "# from smooth_kevin import smoother\n",
    "import py_specrebin\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rc\n",
    "# import pandas as pd\n",
    "path_name = '.'\n",
    "optimized_data_path = path_name + '/Optimized Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wave_600 = np.arange(4000, 11000, .65) \n",
    "new_wave_1200 = np.arange(6000, 11000, .33) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(file_names,mask_name):\n",
    "    \n",
    "    tot_flux = []\n",
    "    tot_wave = []\n",
    "    tot_ivar = []\n",
    "    \n",
    "    for j in range(len(file_names)):\n",
    "        #read in star data\n",
    "        h_star = fits.open(path_name + '/' + 'data/{0}'.format(mask_name) + '/' + file_names[j], ignore_missing_end = True)\n",
    "        \n",
    "        data_star1 = h_star[1].data\n",
    "        star_flux1 = data_star1['SKYSPEC'][0]\n",
    "        star_wave1 = data_star1['LAMBDA'][0]\n",
    "        star_ivar1 = data_star1['IVAR'][0]\n",
    "        \n",
    "        data_star2 = h_star[2].data\n",
    "        star_flux2 = data_star2['SKYSPEC'][0]\n",
    "        star_wave2 = data_star2['LAMBDA'][0]\n",
    "        star_ivar2 = data_star2['IVAR'][0]\n",
    "        \n",
    "        \n",
    "        #combine the blue and red side into one list\n",
    "        star_flux = np.array(list(star_flux1) + list(star_flux2))\n",
    "        star_wave = np.array(list(star_wave1) + list(star_wave2))\n",
    "        star_ivar = np.array(list(star_ivar1) + list(star_ivar2))\n",
    "        \n",
    "        if (sum(star_flux) and sum(star_ivar) and sum(star_wave)) == 0:\n",
    "            file_name_split = file_names[j].split(\".\")\n",
    "            serendip_file_name = \"{0}.{1}.{2}.serendip1.{3}.{4}\".format(file_name_split[0],file_name_split[1],\n",
    "                                                                   file_name_split[2],file_name_split[4],file_name_split[5])\n",
    "            path_to_serendip = fits.open(path_name + '/' + \"data/{0}/{1}\".format(mask_name,serendip_file_name))\n",
    "            \n",
    "            star_flux1_serendip = path_to_serendip[1].data[\"SKYSPEC\"][0]\n",
    "            star_flux2_serendip = path_to_serendip[2].data[\"SKYSPEC\"][0]\n",
    "            star_flux_serendip = np.concatenate((star_flux1_serendip,star_flux2_serendip))\n",
    "            \n",
    "            star_ivar1_serendip = path_to_serendip[1].data[\"IVAR\"][0]\n",
    "            star_ivar2_serendip = path_to_serendip[2].data[\"IVAR\"][0]\n",
    "            star_ivar_serendip = np.concatenate((star_ivar1_serendip,star_ivar2_serendip))\n",
    "            \n",
    "            star_wave1_serendip = path_to_serendip[1].data[\"LAMBDA\"][0]\n",
    "            star_wave2_serendip = path_to_serendip[2].data[\"LAMBDA\"][0]\n",
    "            star_wave_serendip = np.concatenate((star_wave1_serendip,star_wave2_serendip))\n",
    "            \n",
    "            tot_flux.append(star_flux_serendip)\n",
    "            tot_wave.append(star_wave_serendip)\n",
    "            tot_ivar.append(star_ivar_serendip)\n",
    "            \n",
    "            h_star.close()\n",
    "        \n",
    "        else:\n",
    "            #add to above lists\n",
    "            tot_flux.append(star_flux)\n",
    "            tot_wave.append(star_wave)\n",
    "            tot_ivar.append(star_ivar)\n",
    "\n",
    "            h_star.close()\n",
    "        \n",
    "    return tot_flux, tot_wave, tot_ivar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin(fluxes, waves, ivar, grating):\n",
    "    \n",
    "    rbflux = []\n",
    "    rbivar = []\n",
    "    \n",
    "    if grating == 600:\n",
    "        new_wave = new_wave_600\n",
    "    elif grating == 1200:\n",
    "        new_wave = new_wave_1200\n",
    "    \n",
    "    for i in range(len(waves)):\n",
    "        new_flux,new_ivar = py_specrebin.rebinspec(waves[i],fluxes[i],new_wave,ivar=ivar[i])\n",
    "#         new_flux_err = 1/np.sqrt(new_ivar)\n",
    "\n",
    "        rbflux.append(new_flux)\n",
    "        rbivar.append(new_ivar)\n",
    "        \n",
    "    return rbflux, new_wave, rbivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(rebinned_flux_array):\n",
    "    \n",
    "    median_vals = []\n",
    "    \n",
    "    print(len(rebinned_flux_array))\n",
    "    \n",
    "    for i in range(len(rebinned_flux_array[0])):\n",
    "\n",
    "        comp = []\n",
    "        \n",
    "        for array in rebinned_flux_array:\n",
    "            \n",
    "            if np.isfinite(array[i]) == True:\n",
    "                comp.append(array[i])\n",
    "                \n",
    "        median_vals.append(np.median(comp))\n",
    "        \n",
    "    return median_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exclusions():\n",
    "    filepath = 'ISM_EM_LINES.txt'\n",
    "    fp = open(filepath)\n",
    "    all_data = []\n",
    "    for line in (fp):\n",
    "        mask_name = line.split(':')[0].split('_')[0]\n",
    "        slit_number = line.split(':')[1].strip().split(\" \")[0]\n",
    "        if len(slit_number) == 2:\n",
    "            slit_number = '0' + slit_number\n",
    "        elif len(slit_number) == 1:\n",
    "            slit_number = '00' + slit_number\n",
    "        else:\n",
    "            pass\n",
    "        object_id = line.split(':')[1].strip().split()[1]\n",
    "        data = {}\n",
    "        data['mask_name'] = mask_name\n",
    "        data['slit_number'] = slit_number\n",
    "        data['object_id'] = object_id\n",
    "        all_data.append(data)\n",
    "    return all_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_to_include(folder):\n",
    "    import os\n",
    "    list_of_files_to_include = []\n",
    "    list_of_files_to_exclude = []\n",
    "    serendip_files = []\n",
    "    all_file_names_in_folder = os.listdir('data/{}'.format(folder))\n",
    "    y = len(all_file_names_in_folder)\n",
    "    print(\"The number of files in the folder is {0}\".format(y))\n",
    "    all_data = get_exclusions()\n",
    "    len_all_data = len(all_data)\n",
    "    for n in range(y):\n",
    "        parts_of_file_name = all_file_names_in_folder[n].split(\".\")\n",
    "        if parts_of_file_name[0] == 'spec1d': # avoids hidden DS_Store files on my mac\n",
    "            object_id = parts_of_file_name[3]\n",
    "            slit_number = parts_of_file_name[2]\n",
    "            mask_name = parts_of_file_name[1]\n",
    "            should_include = True\n",
    "            should_exclude = True\n",
    "            for k in range(len_all_data):\n",
    "                if ((object_id == all_data[k]['object_id']) and (slit_number == all_data[k]['slit_number']) and (mask_name == all_data[k]['mask_name'])):\n",
    "                    should_include = False\n",
    "                    should_exclude = True\n",
    "                if 'serendip' in object_id:\n",
    "                    should_include = False\n",
    "                    should_exclude = False\n",
    "            if should_include == True:\n",
    "                list_of_files_to_include.append(all_file_names_in_folder[n])       \n",
    "            elif should_exclude == True:\n",
    "                list_of_files_to_exclude.append(all_file_names_in_folder[n])\n",
    "            elif should_include == False & should_exclude == False:\n",
    "                serendip_files.append(all_file_names_in_folder[n])\n",
    "    \n",
    "    print('The number of files left after exclusions is {0}'.format(len(list_of_files_to_include)))\n",
    "    \n",
    "    return sorted(list_of_files_to_include), sorted(list_of_files_to_exclude), sorted(serendip_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function to Save The Rebinned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarthak's function as modified by Liv Gaunt\n",
    "def exportToFits(rbflux, rbwave, rbivar, mask_name, file_names, incl_or_excl):\n",
    "\n",
    "    for i in range(len(rbflux)):\n",
    "            \n",
    "        hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        hdu1.header['INCLUDE'] = (incl_or_excl, 'Include in median calc if T') #this sets the tag for inclusion\n",
    "            \n",
    "        c1 = fits.Column(name='RBFLUX', array=rbflux[i], format='E')\n",
    "        c2 = fits.Column(name='RBWAVE', array=rbwave, format='E') #no [i] on rbwave since it's just one array\n",
    "        c3 = fits.Column(name='RBIVAR', array=rbivar[i], format='E')\n",
    "        hdu2 = fits.BinTableHDU.from_columns([c1, c2, c3]) #first extensional HDU (w data)\n",
    "            \n",
    "        hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "            \n",
    "        #this part puts the files to include in one folder, and those to exclude in another\n",
    "        if incl_or_excl == True:\n",
    "            hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Included'.format(mask_name) + '/' + 'rebinned_{0}'.format(file_names[i]))\n",
    "            \n",
    "        elif incl_or_excl == False:\n",
    "            hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Excluded'.format(mask_name) + '/' + 'rebinned_{0}'.format(file_names[i]))\n",
    "                \n",
    "        else:\n",
    "            hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Serendip'.format(mask_name) + '/' + 'rebinned_{0}'.format(file_names[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function to Read FITS File and Get Back Rebin Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fits_rebinned_data (mask_name, file_names, incl_or_excl):\n",
    "    if incl_or_excl == True:\n",
    "        \n",
    "        #all of these are libraries\n",
    "        \n",
    "        rbflux = []\n",
    "        rbwave = []\n",
    "        rbivar = []\n",
    "        \n",
    "        for slit in file_names: \n",
    "            rb_fits_include = fits.open(optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Rebinned/{0}_Included/rebinned_{1}\".format(mask_name,slit))\n",
    "            rbflux.append(rb_fits_include[1].data[\"RBFLUX\"])\n",
    "            rbwave.append(rb_fits_include[1].data[\"RBWAVE\"])\n",
    "            rbivar.append(rb_fits_include[1].data[\"RBIVAR\"])\n",
    "            \n",
    "    elif incl_or_excl == False: \n",
    "        \n",
    "        rbflux = []\n",
    "        rbwave = []\n",
    "        rbivar = []\n",
    "        \n",
    "        for slit in file_names: \n",
    "            rb_fits_include = fits.open(optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Rebinned/{0}_Excluded/rebinned_{1}\".format(mask_name,slit))\n",
    "            rbflux.append(rb_fits_include[1].data[\"RBFLUX\"])\n",
    "            rbwave.append(rb_fits_include[1].data[\"RBWAVE\"])\n",
    "            rbivar.append(rb_fits_include[1].data[\"RBIVAR\"]) \n",
    "        \n",
    "    else: \n",
    "        \n",
    "        rbflux = []\n",
    "        rbwave = []\n",
    "        rbivar = []\n",
    "        \n",
    "        for slit in file_names: \n",
    "            rb_fits_include = fits.open(optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Rebinned/{0}_Serendip/rebinned_{1}\".format(mask_name,slit))\n",
    "            rbflux.append(rb_fits_include[1].data[\"RBFLUX\"])\n",
    "            rbwave.append(rb_fits_include[1].data[\"RBWAVE\"])\n",
    "            rbivar.append(rb_fits_include[1].data[\"RBIVAR\"]) \n",
    "        \n",
    "    return rbflux, rbwave, rbivar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function to Save The Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportToFitsMedian(median,mask_name):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "        \n",
    "    c1 = fits.Column(name='MEDIAN',array=median,format=\"E\")\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1])\n",
    "        \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "        \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Median/Median_of_{0}.fits.gz'.format(mask_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function To Read and Get Back Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_med_from_fits(mask_name):\n",
    "    median_read = fits.open(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Median/Median_of_{0}.fits.gz'.format(mask_name))\n",
    "    median_fits = median_read[1].data[\"MEDIAN\"] #contain the median \n",
    "    return median_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Median Airglow Subtraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_subtraction(slit_index,rebinned_flux):\n",
    "    \n",
    "    new_flux = []\n",
    "    \n",
    "    spectrum = rebinned_flux[slit_index]\n",
    "   \n",
    "    for i in range(len(spectrum)):\n",
    "        if np.isfinite(spectrum[i]) == True:\n",
    "            new_flux.append(spectrum[i] - median[i])\n",
    "        else:\n",
    "            new_flux.append(spectrum[i])\n",
    "            \n",
    "    return new_flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slit_nums(files):\n",
    "    \n",
    "    slit_nums = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "    \n",
    "        for i in range(len(files)):\n",
    "            parts_of_file_name = files[i].split(\".\")\n",
    "            slit_num = parts_of_file_name[2]\n",
    "            slit_nums.append(int(slit_num))\n",
    "            \n",
    "    return slit_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_slit_index(slit_nums,slit_num): \n",
    "    #print('The index of slit number {} is: '.format(slit_num), slit_nums.index(slit_num))\n",
    "    return slit_nums.index(slit_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(mask_name, slit_nums, rebinned_flux, median, incl_or_excl):\n",
    "    \n",
    "    if incl_or_excl == True:\n",
    "        for slit in slit_nums:\n",
    "            slit_index = find_slit_index(slit_nums,slit)\n",
    "            new_flux = median_subtraction(slit_index,rebinned_flux)\n",
    "            fig,axs = plt.subplots(1)\n",
    "            fig.patch.set_alpha(1)\n",
    "            plt.ylim(-10000,10000) #could try getting a smaller y limit and getting rid of legend\n",
    "            plt.xlim(4000,11000)\n",
    "            #plt.plot(wave_all[slit_index], flux_all[slit_index], color = 'r', label = 'original')\n",
    "            plt.plot(new_wave_600, median, scalex=False, scaley=False, color = 'r', label = 'median')\n",
    "            plt.plot(new_wave_600, rebinned_flux[slit_index] + 4000, scalex=False, scaley=False, color = 'b', label = 'rebinned')\n",
    "            plt.plot(new_wave_600, np.array(new_flux) + 8000, scalex=False, scaley=False, color = 'g', label = 'median subtracted')\n",
    "            plt.title('{} Slit {}'.format(mask_name, slit))\n",
    "            plt.xlabel('Wavelength')\n",
    "            plt.ylabel('Flux')\n",
    "            plt.legend()\n",
    "            fig.savefig('{0}_Spectra/{0}_Included/{0} Slit {1}'.format(mask_name, slit)) #folder name would need to change for each mask\n",
    "    else:\n",
    "        for slit in slit_nums:\n",
    "            slit_index = find_slit_index(slit_nums,slit)\n",
    "            new_flux = median_subtraction(slit_index,rebinned_flux)\n",
    "            fig,axs = plt.subplots(1)\n",
    "            fig.patch.set_alpha(1)\n",
    "            plt.ylim(-10000,10000) #could try getting a smaller y limit and getting rid of legend\n",
    "            plt.xlim(4000,11000)\n",
    "            #plt.plot(wave_all[slit_index], flux_all[slit_index], color = 'r', label = 'original')\n",
    "            plt.plot(new_wave_600, median, scalex=False, scaley=False, color = 'r', label = 'median')\n",
    "            plt.plot(new_wave_600, rebinned_flux[slit_index] + 4000, scalex=False, scaley=False, color = 'b', label = 'rebinned')\n",
    "            plt.plot(new_wave_600, np.array(new_flux) + 8000, scalex=False, scaley=False, color = 'g', label = 'median subtracted')\n",
    "            plt.title('{} Slit {}'.format(mask_name, slit))\n",
    "            plt.xlabel('Wavelength')\n",
    "            plt.ylabel('Flux')\n",
    "            plt.legend()\n",
    "            fig.savefig('{0}_Spectra/{0}_Excluded/{0} Slit {1}'.format(mask_name, slit)) #folder name would need to change for each mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Looking At Specific Slit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_slit(slit_number,rebinned_flux,median,incl_or_excl,multiplier,\n",
    "             min_ylim,max_ylim,min_xlim,max_xlim): #combination of median_subtraction + plotting function \n",
    "#slit_number, rebinned_flux, and incl_excl will need to be changed depending on whether we want to look at incl or excl \n",
    "    if incl_or_excl == True: #median subtraction \n",
    "        new_flux = [] #sky subtracted spectra \n",
    "        slit_index = find_slit_index(slit_nums,slit_number)\n",
    "        spectrum = rebinned_flux[slit_index]\n",
    "        multiplier = multiplier \n",
    "\n",
    "        for i in range(len(spectrum)):\n",
    "            if np.isfinite(spectrum[i]) == True:\n",
    "                new_flux.append((spectrum[i]*multiplier) - median[i]) \n",
    "            else:\n",
    "                new_flux.append(spectrum[i])\n",
    "\n",
    "        #plotting \n",
    "        fig,axs = plt.subplots(1)\n",
    "        plt.ylim(-10000,10000) #could try getting a smaller y limit and getting rid of legend\n",
    "        plt.xlim(4000,11000)\n",
    "        plt.plot(new_wave_600, median, c=\"r\", scalex=False, scaley=False, label = \"median\")\n",
    "        plt.plot(new_wave_600, rebinned_flux[slit_index] + 5000, c=\"b\", scalex=False, scaley=False, label = \"rebinned\")\n",
    "        #plt.plot(new_wave_600, rebinned_flux[\"slit_{}\".format(slit_number)] - 0.9*median + 8000, scalex=False, scaley=False,\n",
    "                #label = \"subtracted\") #error\n",
    "        plt.plot(new_wave_600, np.array(new_flux), c=\"g\", scalex=False, scaley=False,label = \"subtracted\")\n",
    "        plt.xlabel(\"Wavelength (A)\")\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "        plt.title(\"Slit #{}\".format(slit_number))\n",
    "        \n",
    "    else:\n",
    "        new_flux = [] #sky subtracted spectra w/ scaling\n",
    "        new_flux_no_scaling = [] #sky subtracted spectra w/o scaling\n",
    "        rbflux_with_scaling = []\n",
    "        slit_index = find_slit_index(slit_nums_exclude,slit_number) #changed slit_nums to slit_nums_exclude\n",
    "        spectrum = rebinned_flux[slit_index] #rbflux w/ no scaling\n",
    "        multiplier = multiplier \n",
    "\n",
    "        #multiplying the rbflux by scaling factor \n",
    "        for i in range(len(spectrum)):\n",
    "            if np.isfinite(spectrum[i]) == True:\n",
    "                new_flux.append((spectrum[i] * multiplier) - median[i]) \n",
    "                rbflux_with_scaling.append((spectrum[i] * multiplier))\n",
    "            else:\n",
    "                new_flux.append(spectrum[i])\n",
    "                rbflux_with_scaling.append(spectrum[i])\n",
    "\n",
    "        #no scaling factor\n",
    "        for i in range(len(spectrum)):\n",
    "            if np.isfinite(spectrum[i]) == True:\n",
    "                new_flux_no_scaling.append((spectrum[i]) - median[i]) \n",
    "            else:\n",
    "                new_flux_no_scaling.append(spectrum[i])\n",
    "        \n",
    "        #plotting \n",
    "        fig,axs = plt.subplots(1)\n",
    "        plt.ylim(min_ylim,max_ylim) #could try getting a smaller y limit and getting rid of legend\n",
    "        plt.xlim(min_xlim,max_xlim)\n",
    "        plt.plot(new_wave_600, median, c=\"r\", scalex=False, scaley=False, label = \"median\")\n",
    "        plt.plot(new_wave_600, rbflux_with_scaling, c=\"b\", scalex=False, scaley=False, label = \"rebinned w/ scaling\")\n",
    "        plt.plot(new_wave_600, np.array(new_flux_no_scaling), c=\"purple\", scalex=False, scaley=False, label = \"Subtracted w/o scaling\")\n",
    "        plt.plot(new_wave_600, np.array(new_flux), c=\"g\", scalex=False, scaley=False,label = \"subtracted\")\n",
    "        plt.xlabel(\"Wavelength (A)\")\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "        plt.title(\"Slit #{}\".format(slit_number) + \"/Multiplier: {}\".format(multiplier) \n",
    "                  + \"/From {0} to {1}\".format(min_xlim,max_xlim))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define The Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_name = \"M33D2A\" #change to fit the appropriate mask "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Files We Want to Include and Exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering files\n",
    "list_of_files_to_include, list_of_files_to_exclude, list_of_serendip_files = get_files_to_include(mask_name)\n",
    "\n",
    "#sorted\n",
    "#file_names = all slits used to create the median (airglow)\n",
    "#file_names_exclude = all slits that contain ISM emission lines \n",
    "#file_names_serendip = all serendip files\n",
    "#file_names_all = all slits excluding \"serendip\"\n",
    "\n",
    "file_names = list_of_files_to_include\n",
    "file_names_exclude = list_of_files_to_exclude\n",
    "file_names_serendip = list_of_serendip_files\n",
    "file_names_all = list_of_files_to_include + list_of_files_to_exclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracting The Wavelength, Flux, and Inverse Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to comment out the codes in this section after you have rebinned and saved your data!!!\n",
    "\n",
    "Then make sure to uncomment them whenever you're working with a new mask and want to rebin!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data\n",
    "#try getting and rebinning all files\n",
    "flux, wave, ivar = get_original_data(file_names, mask_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the original data\n",
    "rbflux, rbwave, rbivar = rebin(flux, wave, ivar, 600) # this takes about 4 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all excluded data\n",
    "flux_exclude, wave_exclude, ivar_exclude = get_original_data(file_names_exclude, mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the excluded data\n",
    "rbflux_exclude, rbwave_exclude, rbivar_exclude = rebin(flux_exclude, wave_exclude, ivar_exclude, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all serendip data \n",
    "#NOTE: we will never use it but is good to just process it\n",
    "flux_serendip, wave_serendip, ivar_serendip = get_original_data(list_of_serendip_files, mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the serendip data\n",
    "rbflux_serendip, rbwave_serendip, rbivar_serendip = rebin(flux_serendip, wave_serendip, ivar_serendip, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving The Rebinned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [#make three folders to store the rebinned data, the median, and the spectra\n",
    "        \"{0}/{1}/{1}_Rebinned\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Spectra\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Median\".format(optimized_data_path,mask_name),\n",
    "\n",
    "        #make sub-folders for rebinned data\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Excluded\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Included\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Serendip\".format(optimized_data_path,mask_name),\n",
    "\n",
    "        #make sub-folders for the spectra\n",
    "        \"{0}/{1}/{1}_Spectra/{1}_Excluded\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Spectra/{1}_Included\".format(optimized_data_path,mask_name),\n",
    "\n",
    "        #make directory to stores the scaled flux and shifted wavelength and polynomial coefficients\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Scale_Values\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Shift_Values\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Polynomial_Coefficients\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Polynomial_Coefficients/{1}_Shifting_Polynomial_Coefficients\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Polynomial_Coefficients/{1}_Scaling_Polynomial_Coefficients\".format(optimized_data_path,mask_name),\n",
    "    \n",
    "        #make a directory to store new median\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_New_Median\".format(optimized_data_path,mask_name),\n",
    "\n",
    "        #make sub-folders for scaling and shifting factors and rebinned flux w/ shifted wavelength\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Shifting_Factor\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Scaling_Factor\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Scaling_and_Shifting_Factor\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Rebinned/{1}_Rebinned_Flux_Shifted_Wave\".format(optimized_data_path,mask_name),\n",
    "\n",
    "        #make directories to stores final trimmed spectra\n",
    "        \"{0}/{1}/{1}_Trimmed_Spectra\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Trimmed_Spectra/Excluded\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Trimmed_Spectra/Included\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Trimmed_Spectra/Optimized_Spectrum_Flux\".format(optimized_data_path,mask_name),\n",
    "\n",
    "        #make directories to stores polynomial fits and factor vs RMS plots\n",
    "        \"{0}/{1}/{1}_Polynomial_Graph\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Polynomial_Graph/Scaling_vs_RMS\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Polynomial_Graph/Shifting_vs_RMS\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Polynomial_Graph/Scaling_Fitting\".format(optimized_data_path,mask_name),\n",
    "        \"{0}/{1}/{1}_Polynomial_Graph/Shifting_Fitting\".format(optimized_data_path,mask_name)]\n",
    "\n",
    "for path in paths:\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportToFits(rbflux, rbwave, rbivar, mask_name, file_names, True) \n",
    "exportToFits(rbflux_exclude, rbwave_exclude, rbivar_exclude, mask_name, file_names_exclude, False)\n",
    "exportToFits(rbflux_serendip, rbwave_serendip, rbivar_serendip, mask_name, file_names_serendip, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading Back The Rebinned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbflux_fits,rbwave_fits,rbivar_fits = get_fits_rebinned_data(mask_name,file_names,True)\n",
    "rbflux_fits_exclude,rbwave_fits_exclude,rbivar_fits_exclude = get_fits_rebinned_data(mask_name,file_names_exclude,False)\n",
    "rbflux_fits_serendip,rbwave_fits_serendip,rbivar_fits_serendip = get_fits_rebinned_data(mask_name,file_names_serendip,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finding The Median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking the median\n",
    "median = find_median(rbflux_fits) #median length is 10770 (M33D2A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving Median As FITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exportToFitsMedian(median,mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting Back Median From FITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_fits = get_med_from_fits(mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Slits to Include and Exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slit_nums = get_slit_nums(file_names)\n",
    "slit_nums_exclude = get_slit_nums(file_names_exclude)\n",
    "\n",
    "all_slit_nums = get_slit_nums(file_names_all)\n",
    "\n",
    "print(\"Slit # to INCLUDE in median calculation: {0}\".format(slit_nums))\n",
    "print(\"Slit # to EXCLUDE: {0}\".format(slit_nums_exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting ALL Slits In A Mask and Saving It In A Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot all slits and save it as a png in a folder \n",
    "#Need to change the mask name for each mask \n",
    "# plotting(mask_name, slit_nums, rbflux_fits, median_fits, True)\n",
    "# plotting(mask_name, slit_nums_exclude, rbflux_fits_exclude, median_fits, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optimization Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_median = 150 #define the threshold for median boolean\n",
    "threshold_sky_sub = 50 #define the threshold for sky subtraction boolean \n",
    "\n",
    "multipliers = np.arange(0.7,1.3,0.01) #array of multipliers we want to test\n",
    "shift_test_values = np.arange(-0.2, 0.21, 0.01) #range of shift values we want to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean Arrays & Moving Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "\n",
    "def moving_median(a, window=325):\n",
    "    \n",
    "    '''\n",
    "    Returns the moving median values of the array,\n",
    "    using a window of a given size, centered at\n",
    "    each point.\n",
    "    \n",
    "    Version - 4.0\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : ndarray\n",
    "        One dimensional flux array.\n",
    "    window : int, optional\n",
    "        The size of each segment for taking the median.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    median_arr : One dimensional array of moving median.\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    all_indices = np.arange(len(a))\n",
    "    finite_bool = np.isfinite(a)\n",
    "    nan_indices = all_indices[np.invert(finite_bool)]\n",
    "    nan_indices_set = set(nan_indices)\n",
    "    n = len(finite_bool)\n",
    "\n",
    "    if (nan_indices_set=={0,n} or nan_indices_set=={0} or nan_indices_set=={n}):\n",
    "        \n",
    "        finite_indices = all_indices[finite_bool]\n",
    "        nearest_finite_indices = np.searchsorted(finite_indices, nan_indices)\n",
    "        nearest_finite_indices = nearest_finite_indices - (nearest_finite_indices==len(finite_indices))\n",
    "        a[nan_indices] = a[finite_indices[nearest_finite_indices]][:]\n",
    "        median_arr = median_filter(a, window, mode='nearest')\n",
    "\n",
    "    elif (len(nan_indices_set)==0):\n",
    "        \n",
    "        median_arr = np.nan*np.ones(len(a))\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if True not in finite_bool:\n",
    "            median_arr = np.nan*np.ones(len(a))\n",
    "            \n",
    "        else:\n",
    "            finite_indices = all_indices[finite_bool]\n",
    "            nearest_finite_indices = np.searchsorted(finite_indices, nan_indices)\n",
    "            gap_indices = ((nearest_finite_indices>0) & (nearest_finite_indices<len(finite_indices)))\n",
    "            middle_nan_indices = nan_indices[gap_indices]\n",
    "            right_nearest_indices = finite_indices[nearest_finite_indices[gap_indices]]\n",
    "            left_nearest_indices = finite_indices[nearest_finite_indices[gap_indices] - 1]\n",
    "            right_distances = abs(right_nearest_indices - middle_nan_indices)\n",
    "            left_distances = abs(left_nearest_indices - middle_nan_indices)\n",
    "            right_is_near_bool = (left_distances > right_distances)\n",
    "            left_is_near_bool = (left_distances <= right_distances)\n",
    "            a[middle_nan_indices[right_is_near_bool]] = a[right_nearest_indices[right_is_near_bool]][:]\n",
    "            a[middle_nan_indices[left_is_near_bool]] = a[left_nearest_indices[left_is_near_bool]][:]\n",
    "            a[nan_indices[nearest_finite_indices==0]] = a[finite_indices[0]]\n",
    "            a[nan_indices[nearest_finite_indices==len(finite_indices)]] = a[finite_indices[-1]]\n",
    "            median_arr = median_filter(a, window, mode='nearest')\n",
    "    \n",
    "    return (median_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_moving_median = True\n",
    "window = 325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_threshold(median, threshold):\n",
    "    \n",
    "    median_boolean = []\n",
    "    \n",
    "    for value in median:\n",
    "        \n",
    "        if np.isfinite(value) == True: #to filter out nan \n",
    "            \n",
    "            if value > threshold:\n",
    "                median_boolean.append(True)\n",
    "                \n",
    "            elif value <= threshold:\n",
    "                median_boolean.append(False)\n",
    "            \n",
    "        else:\n",
    "            median_boolean.append(False)\n",
    "            \n",
    "    median_boolean_array = np.array(median_boolean)\n",
    "    \n",
    "    return median_boolean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wave_bool(wavelength, min_wave, max_wave):\n",
    "    \n",
    "    wavelength_boolean = []\n",
    "    \n",
    "    for value in wavelength: \n",
    "        \n",
    "        if (value > min_wave) and (value < max_wave): \n",
    "            \n",
    "            wavelength_boolean.append(True)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            wavelength_boolean.append(False)\n",
    "            \n",
    "    wavelength_boolean_array = np.array(wavelength_boolean)\n",
    "            \n",
    "    return wavelength_boolean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sky_sub_bool(slit_index, rebinned_flux_list, median, threshold, use_moving_median=use_moving_median):\n",
    "#     new_flux = [] #sky subtracted spectra \n",
    "    #slit_index = find_slit_index(slit_nums_exclude,slit_number) #changed slit_nums to slit_nums_exclude\n",
    "    slit_index = slit_index\n",
    "    spectrum = rebinned_flux_list[slit_index]\n",
    "\n",
    "    # rbflux - median is stored as new_flux \n",
    "    new_flux = spectrum - median\n",
    "    \n",
    "    if (use_moving_median==True):\n",
    "        new_flux = np.asarray(new_flux) - moving_median(np.asarray(new_flux), window=window)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    rbflux_boolean = new_flux <= threshold\n",
    "            \n",
    "    return np.array(rbflux_boolean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Polynomial Fits and Weighted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_first_order(x,poly_const):\n",
    "    return (x * poly_const[0]) + (poly_const[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_second_order(x,poly_const): #function that represent the third-order polynomial \n",
    "    return (x**2 * poly_const[0]) + (x * poly_const[1]) + (poly_const[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_third_order(x,poly_const): #function that represent the third-order polynomial \n",
    "    return (x**3 * poly_const[0]) + (x**2 * poly_const[1]) + (x * poly_const[2]) + (poly_const[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_fourth_order(x,poly_const): #function that represent the fourth-order polynomial \n",
    "    return (x**4 * poly_const[0]) + (x**3 * poly_const[1]) + (x**2 * poly_const[2]) + (x * poly_const[3]) + (poly_const[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_wave(median,threshold_median,threshold_sky_sub,wavelength,min_wave,max_wave,rbflux,slit_index):\n",
    "    \n",
    "    #boolean array using median and threshold\n",
    "    median_boolean_array = median_threshold(median, threshold_median)\n",
    "    \n",
    "    #boolean array using wavelength\n",
    "    wavelength_boolean_array = create_wave_bool(wavelength, min_wave, max_wave)\n",
    "    \n",
    "    #boolean array using rbflux - median \n",
    "    sky_sub_boolean_array = sky_sub_bool(slit_index,rbflux,median,threshold_sky_sub)\n",
    "    \n",
    "    #multiply the two boolean arrays\n",
    "    multiply_boolean = median_boolean_array * wavelength_boolean_array * sky_sub_boolean_array\n",
    "    \n",
    "    median_values_for_weight = [] #determine the median we want to use as our weight\n",
    "    median_values_index = []\n",
    "    \n",
    "    for n in range(len(multiply_boolean)):\n",
    "        \n",
    "        if multiply_boolean[n] == True:\n",
    "            \n",
    "            median_values_for_weight.append(median[n])\n",
    "            median_values_index.append(n)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            pass\n",
    "    \n",
    "    wavelength_values_for_weight = [] #determine the wavelength we want to use in \n",
    "    \n",
    "    for index in median_values_index:\n",
    "        \n",
    "        wavelength_values_for_weight.append(wavelength[index])\n",
    "    \n",
    "    #calculating the weighted wavelength\n",
    "    if len(median_values_for_weight) == 0:\n",
    "        print(\"All boolean values are False. Weighted wavelength cannot be calculated! (Wavelength {0} to {1})\".format(min_wave,max_wave))\n",
    "    \n",
    "    elif len(median_values_for_weight) == len(wavelength_values_for_weight): #make sure they have same length\n",
    "        \n",
    "        sigma_med_wave = []\n",
    "        \n",
    "        for n in range(len(median_values_for_weight)):\n",
    "            \n",
    "            sigma_med_wave.append(median_values_for_weight[n] * wavelength_values_for_weight[n])\n",
    "        \n",
    "        weighted_wavelength = sum(np.array(sigma_med_wave)) / sum(np.array(median_values_for_weight))\n",
    "        \n",
    "        return weighted_wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automating_weighted_wave_multiple_slits(index_of_slit, fluxes, median):\n",
    "        \n",
    "    #wavelength_array =  np.arange(4000,11200,200)\n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    #wavelength_array = np.arange(4000,11350,350)\n",
    "    \n",
    "    weighted_wavelength_list = []\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "\n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "        \n",
    "            weighted_wavelength = weighted_wave(median, threshold_median,threshold_sky_sub, rbwave_fits[0],wavelength_array[index],\n",
    "                                          wavelength_array[index+1],fluxes,index_of_slit)\n",
    "            \n",
    "            weighted_wavelength_list.append(weighted_wavelength)\n",
    "\n",
    "    #weighted_wavelength_list_final = []\n",
    "    \n",
    "    #for wavelength in weighted_wavelength_list: #filter out any None values \n",
    "        \n",
    "        #if wavelength != None:\n",
    "            \n",
    "            #weighted_wavelength_list_final.append(wavelength)\n",
    "        \n",
    "    return weighted_wavelength_list\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class & Decorator Function To Generate Log Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution:** Execute the following cell only once per run. Do not modify the ```std_out``` or ```std_err``` variables. If they are modified by accident, please restart the kernel and run the notebook from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original streams for stdout and stderr. To be used for logging output later\n",
    "\n",
    "import sys\n",
    "std_out = sys.stdout; std_err = sys.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For duplicating the output stream to log files during the optimization process.\n",
    "\n",
    "class multifile(object):\n",
    "    def __init__(self, files):\n",
    "        self._files = files\n",
    "    def __getattr__(self, attr, *args):\n",
    "        return self._wrap(attr, *args)\n",
    "    def _wrap(self, attr, *args):\n",
    "        def g(*a, **kw):\n",
    "            for f in self._files:\n",
    "                res = getattr(f, attr, *args)(*a, **kw)\n",
    "            return res\n",
    "        return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decorating function to generate log files during shifting, and scaling.\n",
    "\n",
    "def write_to_log(filename):\n",
    "    def inner_decorator_1(func):\n",
    "        def inner_decorator_2(*args, **kwargs):\n",
    "            \n",
    "            try:\n",
    "                log_file = open(filename, 'w')\n",
    "                sys.stdout = multifile([ std_out, log_file ])\n",
    "                sys.stderr = multifile([ std_err, log_file ])\n",
    "\n",
    "                # Function runs here.\n",
    "                x = func(*args, **kwargs)\n",
    "\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "            finally:\n",
    "                log_file.close()\n",
    "                sys.stdout = std_out\n",
    "                sys.stderr = std_err\n",
    "                \n",
    "            return x\n",
    "        return inner_decorator_2\n",
    "    return inner_decorator_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def shifting_wavelength(waves, shifting_value):\n",
    "    \n",
    "# #     waves_shifted = []\n",
    "    \n",
    "# #     for i in range(len(waves)):\n",
    "# #         waves_shifted.append(waves[i] + shifting_value)\n",
    "    \n",
    "#     return waves_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def looping_shifting_wavelength(original_wavelength,test_values,index_of_slit):\n",
    "    \n",
    "#     wave_shifted_dict = {}\n",
    "    \n",
    "#     for value in test_values: \n",
    "#         wave_shifted = shifting_wavelength(original_wavelength[index_of_slit],value)\n",
    "#         wave_shifted_dict[\"Shifted_{}\".format(round(value,2))] = wave_shifted\n",
    "    \n",
    "#     return wave_shifted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin_wave_shifted(flux, wave, ivar, test_values, index_of_slit):\n",
    "    \n",
    "    rbflux_shifted_dict = {}\n",
    "    \n",
    "    for value in test_values:\n",
    "        \n",
    "        rbflux_shifted,rbwave_shifted,rbivar_shifted = rebin([flux[index_of_slit]], \n",
    "                                                             [wave[index_of_slit] + value], \n",
    "                                                             [ivar[index_of_slit]], 600)\n",
    "        \n",
    "        rbflux_shifted_dict[\"Shifted_{}\".format(round(value,2))] = np.asarray(rbflux_shifted).ravel()\n",
    "    \n",
    "    return rbflux_shifted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_flux_subtraction_threshold(rbflux_shifted_dict, median,\n",
    "                                       use_moving_median=use_moving_median):\n",
    "    \n",
    "    keys = list(rbflux_shifted_dict.keys())\n",
    "    sub_dict = {}\n",
    "    sub_boolean_dict = {}\n",
    "    \n",
    "    if use_moving_median==True:\n",
    "        flux_without_shifting = rbflux_shifted_dict['Shifted_0.0']\n",
    "        median_baseline = moving_median(flux_without_shifting-median,  window=window)\n",
    "        for i in keys:\n",
    "            sub_flux = rbflux_shifted_dict[i] - median\n",
    "            sub_flux = sub_flux - median_baseline\n",
    "            sub_dict[i] = sub_flux\n",
    "            sub_boolean_dict[i] = np.asarray(sub_flux < threshold_sky_sub)\n",
    "    else:\n",
    "        for i in keys:\n",
    "            sub_flux = rbflux_shifted_dict[i] - median\n",
    "            sub_dict[i] = sub_flux\n",
    "            sub_boolean_dict[i] = np.asarray(sub_flux < threshold_sky_sub)\n",
    "    \n",
    "    return (sub_dict, sub_boolean_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbflux_shifted_minus_median(sub_dict, sub_boolean_dict,\n",
    "                                median_and_wavelength_boolean_array, shift_test_values):\n",
    "    \n",
    "    subtraction_dict_shifted = {}\n",
    "    \n",
    "    for test_value in shift_test_values:\n",
    "        \n",
    "        key = \"Shifted_{}\".format(round(test_value,2))\n",
    "        \n",
    "        multiply_boolean = sub_boolean_dict[key] * median_and_wavelength_boolean_array\n",
    "        subtraction_full_array = sub_dict[key]\n",
    "        subtraction_slice = subtraction_full_array[multiply_boolean]\n",
    "                      \n",
    "        subtraction_dict_shifted[key] = subtraction_slice \n",
    "    \n",
    "    return subtraction_dict_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_calculation_shift(rms_dict_sorted_shift, shift_test_values):\n",
    "    \n",
    "    rms_dict_shift = {}\n",
    "\n",
    "    for test_value in shift_test_values:\n",
    "\n",
    "        values_for_rms_cal = rms_dict_sorted_shift[\"Shifted_{}\".format(round(test_value,2))]\n",
    "        \n",
    "        values_for_rms_cal = np.array(values_for_rms_cal)\n",
    "        rms = np.nanmean(values_for_rms_cal*values_for_rms_cal)**0.5\n",
    "#         rms = statistics.stdev(values_for_rms_cal)\n",
    "\n",
    "        rms_dict_shift[\"Shifted_{}\".format(round(test_value,2))] = rms\n",
    "        \n",
    "    return rms_dict_shift\n",
    "    \n",
    "    \n",
    "        #print(\"Everything is False. There's no True boolean. Therefore, RMS cannot be calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_the_shift_rms(slit_number,shift_test_values, rms_dict_shift, min_wave, max_wave):\n",
    "    \n",
    "    value_list = []\n",
    "    path = optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Polynomial_Graph/Shifting_vs_RMS/Slit_{1}\".format(mask_name,slit_number)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    for test_value in shift_test_values: \n",
    "\n",
    "        value_list.append(rms_dict_shift[\"Shifted_{0}\".format(round(test_value,2))])\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    fig.patch.set_alpha(1)\n",
    "    plt.plot(shift_test_values,value_list)\n",
    "    plt.xlabel(\"Shifting\")\n",
    "    plt.ylabel(\"RMS\")\n",
    "    plt.title(\"Mask {0}: Slit #{1}\\nRMS vs Shifting ({2} A to {3} A)\".format(mask_name,slit_number,min_wave,max_wave))\n",
    "    fig.savefig(path+'/{0}_Slit_{1}_{2}_to_{3}.png'.format(mask_name,slit_number,min_wave,max_wave))\n",
    "        \n",
    "    min_val = min(value_list)\n",
    "\n",
    "    shifting_value = round(shift_test_values[value_list.index(min_val)],2)\n",
    "\n",
    "    print(\"Shifting w/ minimum RMS (Slit #{0}): {1}\".format(slit_number,shifting_value) + \" ({0} A to {1} A)\".format(min_wave,max_wave))\n",
    "\n",
    "    return shifting_value\n",
    "        \n",
    "    \n",
    "        #print(\"Because we have no RMS there is no plot.\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating All The Functions Used In Shifting Process (For A Single Slit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_shifting(slit_number, flux, wave, ivar,\n",
    "                     median_boolean_array, wavelength, slit_index):\n",
    "    \n",
    "    #define all values we want to test\n",
    "    # shift_test_values = shift_test_values\n",
    "    \n",
    "    #shift original wavelength by test values\n",
    "    #store everything as a dictionary. Format: \"Shifted_(test_value):[shifted wavelength]\"\n",
    "#     wave_shifted_dict = looping_shifting_wavelength(wave,shift_test_values,slit_index) \n",
    "    \n",
    "    #rebin using shifted wavelength \n",
    "    #store rebinned flux as a dictionary. Format: \" Shifted_(test_value):[rebinned flux]\"\n",
    "    rbflux_shifted_dict = rebin_wave_shifted(flux, wave, ivar, shift_test_values, slit_index)\n",
    "    \n",
    "    sub_dict, sub_boolean_dict = shifted_flux_subtraction_threshold(rbflux_shifted_dict, median)\n",
    "    \n",
    "    #500 A segments \n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    \n",
    "    shifting_value_dict = {}\n",
    "    \n",
    "    #for loop to find the multiply_boolean and median-subtracted spectrum\n",
    "    for n in range(len(wavelength_array)):\n",
    "\n",
    "        if (n + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "            #boolean array using wavelength\n",
    "            wavelength_boolean_array = create_wave_bool(wavelength, wavelength_array[n], wavelength_array[n+1])\n",
    "\n",
    "            #boolean array using rbflux - median \n",
    "#             sky_sub_boolean_array = sky_sub_bool(slit_index,rbflux_list,median,threshold_sky_sub)\n",
    "            \n",
    "            median_and_wavelength_boolean_array = median_boolean_array * wavelength_boolean_array\n",
    "\n",
    "#             #multiply the three boolean arrays\n",
    "#             multiply_boolean = median_boolean_array * wavelength_boolean_array * sky_sub_boolean_array\n",
    "            \n",
    "            #use the rebinned flux, the optimal scaling factor, and median to calculate the median-subtracted spectrum\n",
    "            subtraction_dict_shifted = rbflux_shifted_minus_median(sub_dict, sub_boolean_dict,\n",
    "                                                                    median_and_wavelength_boolean_array, shift_test_values)\n",
    "            \n",
    "            if len(subtraction_dict_shifted[\"Shifted_0.2\"]) == 0 or len(subtraction_dict_shifted[\"Shifted_0.2\"]) == 1:\n",
    "                shifting_value_dict[\"{0}_to_{1}\".format(wavelength_array[n],wavelength_array[n+1])] = None\n",
    "                print(\"Boolean are all False. No values can be use to calculate the RMS. From {0} to {1}\".format(wavelength_array[n],wavelength_array[n+1]))\n",
    "                \n",
    "            else: \n",
    "            \n",
    "                #rms values\n",
    "                rms_dict_shift = rms_calculation_shift(subtraction_dict_shifted,shift_test_values)\n",
    "\n",
    "                #rms values that will be used \n",
    "                shifting_value = plotting_the_shift_rms(slit_number,shift_test_values,rms_dict_shift,wavelength_array[n],wavelength_array[n+1])\n",
    "\n",
    "                shifting_value_dict[\"{0}_to_{1}\".format(wavelength_array[n],wavelength_array[n+1])] = shifting_value\n",
    "            \n",
    "    return shifting_value_dict, rbflux_shifted_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Flux Rebinned With Shifted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving rbflux_shifted_dict as a nyp files, take a bit of space, need to reduce it.\n",
    "#get only the rbflux that we will need \n",
    "def sorting_needed_shift_factor(shifting_value_dict,rbflux_shifted_dict):\n",
    "    \n",
    "    original_shift_factor_list = list(shifting_value_dict.values())\n",
    "    \n",
    "    #remove all duplicate and all None \n",
    "    remove_duplicate_and_none = list(set(original_shift_factor_list))\n",
    "    \n",
    "    #selects only values that are optimal shift factor\n",
    "    sorted_rbflux_shifted_dict = {}\n",
    "    \n",
    "    for value in remove_duplicate_and_none:\n",
    "        if value == None:\n",
    "            remove_duplicate_and_none.remove(value)\n",
    "            \n",
    "    for value in remove_duplicate_and_none:\n",
    "        sorted_rbflux_shifted_dict[\"Shifted_{0}\".format(value)] = rbflux_shifted_dict[\"Shifted_{0}\".format(value)]\n",
    "            \n",
    "    return(sorted_rbflux_shifted_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note sure how to save dictionary as FITS file, therefore, use npy file\n",
    "def saving_rbflux_shifted(mask_name,slit_number_used,sorted_rbflux_shifted_dict):\n",
    "    \n",
    "    #saving the dictionary as a npy file\n",
    "    np.save(\"{2}/{0}/{0}_Rebinned/{0}_Rebinned_Flux_Shifted_Wave/rbflux_shifted_dict_{0}_{1}.npy\".format(mask_name,slit_number_used,optimized_data_path),sorted_rbflux_shifted_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit for Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelength_shifting_function(shifting_value_dict, weighted_wavelength): #used to make a plot of optimal shifting factor vs wavelength\n",
    "    \n",
    "    shifting_values = [] #do not plot any shifting value that has None\n",
    "    \n",
    "    wavelength_values = [] #contains all the wavelength we will plots \n",
    "    \n",
    "    for index in range(len(shifting_value_dict.values())): #filtering out the None values\n",
    "        \n",
    "        if list(shifting_value_dict.values())[index] != None:\n",
    "        \n",
    "            shifting_values.append(list(shifting_value_dict.values())[index])\n",
    "            \n",
    "            wavelength_values.append(weighted_wavelength[index])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "\n",
    "    #finding the polynomial constant for second order\n",
    "    poly_const_second_deg = np.polyfit(wavelength_values,shifting_values,2)\n",
    "    print(\"Second order polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    #finding the polynomial constant for third order\n",
    "    poly_const_third_deg = np.polyfit(wavelength_values,shifting_values,3)\n",
    "    print(\"Third order polynomial: y = {0}(x^3) + {1}(x^2) + {2}(x) + {3}\".format(poly_const_third_deg[0],\n",
    "                                                                                  poly_const_third_deg[1],poly_const_third_deg[2],poly_const_third_deg[3]))\n",
    "    \n",
    "    #finding the polynomial constant for fourth order\n",
    "    poly_const_fourth_deg = np.polyfit(wavelength_values,shifting_values,4)\n",
    "    print(\"Fourth order polynomial: y = {0}(x^4) + {1}(x^3) + {2}(x^2) + {3}(x) + {4}\".format(poly_const_fourth_deg[0],\n",
    "                                                                                  poly_const_fourth_deg[1],poly_const_fourth_deg[2],poly_const_fourth_deg[3]\n",
    "                                                                                             ,poly_const_fourth_deg[4]))\n",
    "    \n",
    "    \n",
    "    #calculating the third and fourth order polynomial as a function of wavelength\n",
    "    poly_second_order = []\n",
    "    \n",
    "    poly_third_order = []\n",
    "    \n",
    "    poly_fourth_order = []\n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "        poly_third_order.append(polynomial_third_order(value,poly_const_third_deg))\n",
    "        \n",
    "        poly_fourth_order.append(polynomial_fourth_order(value,poly_const_fourth_deg))\n",
    "                                                                                            \n",
    "    #plotting the optimal shifting factors, third-order polynomial, fourth-order polynomial as a function of wavelength\n",
    "    fig,axs = plt.subplots(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.25,0.25)\n",
    "    axs.set_title(\"Optimal Shifting Factor vs Wavelength\")\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Shifting Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.plot(all_wavelength_values,poly_third_order,scalex=False,scaley=False,label=\"Third-Order\",c=\"blue\")\n",
    "    axs.plot(all_wavelength_values,poly_fourth_order,scalex=False,scaley=False,label=\"Fourth-Order\",c=\"orange\")\n",
    "    axs.scatter(wavelength_values, shifting_values,label=\"Optimal Shifting Factor\",c=\"black\")\n",
    "    axs.legend()\n",
    "    \n",
    "    return shifting_values,wavelength_values,poly_const_second_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving The Polymial Fits (Shifting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_sigma_clip_shifting(shifting_values_list,weighted_wavelength_list,poly_const_second_deg):\n",
    "    \n",
    "    #optimal shifting value - optimal shifting value based on line of best fit\n",
    "    deviation = [] \n",
    "    \n",
    "    #determine the vertical difference (deviation) between dots and line of best fit\n",
    "    for n in range(len(weighted_wavelength_list)):\n",
    "        deviation.append(np.abs(polynomial_second_order(weighted_wavelength_list[n],poly_const_second_deg) - shifting_values_list[n]))\n",
    "    \n",
    "    print(\"First Deviation: {}\".format(deviation))\n",
    "    print(\"First Deviation RMS: {}\".format(statistics.stdev(deviation)))\n",
    "    \n",
    "    #BEGIN FIRST ITERATION\n",
    "    \n",
    "    #first iteration \n",
    "    shifting_values_1 = []\n",
    "    wavelength_values_1 = []\n",
    "    \n",
    "    #to store outlier\n",
    "    outlier_deviation = []\n",
    "    \n",
    "    #remove outliers from data\n",
    "    for n in range(len(deviation)):\n",
    "        if deviation[n]/statistics.stdev(deviation) < 3.5:\n",
    "            shifting_values_1.append(shifting_values_list[n])#keep all non-outlier\n",
    "            wavelength_values_1.append(weighted_wavelength_list[n])\n",
    "        #add all outliers to a separate list\n",
    "        else: \n",
    "            outlier_deviation.append(deviation[n])\n",
    "    \n",
    "    #if there's no outlier, return inputs \n",
    "    if len(outlier_deviation) == 0: \n",
    "        return shifting_values_1, wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(outlier_deviation) > 1: \n",
    "        outlier_deviation.remove(max(outlier_deviation))\n",
    "        for value in outlier_deviation:\n",
    "            shifting_values_1.append(shifting_values_list[deviation.index(value)])\n",
    "            wavelength_values_1.append(weighted_wavelength_list[deviation.index(value)])\n",
    "     \n",
    "    #if there is only one values left after the outliers is removed, return shifting values and wavelength\n",
    "    if len(shifting_values_1) == 0 or len(shifting_values_1) == 1:\n",
    "        return shifting_values_1,wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    #PERFORM A NEW FITTING\n",
    "    \n",
    "    new_poly_coeff = np.polyfit(wavelength_values_1,shifting_values_1,2)\n",
    "    \n",
    "    \n",
    "    #CALCULATE NEW DEVIATION AND REMOVE OUTLIERS\n",
    "    \n",
    "    new_deviation = []\n",
    "    \n",
    "    for n in range(len(wavelength_values_1)):\n",
    "        new_deviation.append(np.abs(polynomial_second_order(wavelength_values_1[n],new_poly_coeff) - shifting_values_1[n]))\n",
    "    \n",
    "    print(\"Second Deviation: {}\".format(new_deviation))\n",
    "    print(\"Second Deviation RMS: {}\".format(statistics.stdev(new_deviation)))\n",
    "                             \n",
    "    #final set of data with outliers removed\n",
    "    no_outlier_shifting_values_list = []\n",
    "    no_outlier_wavelength_values_list = []\n",
    "    \n",
    "    #to stores outliers\n",
    "    new_outlier_deviation = []\n",
    "    \n",
    "    #remove outliers\n",
    "    for n in range(len(new_deviation)):\n",
    "        if new_deviation[n]/statistics.stdev(new_deviation) < 3.5:\n",
    "            no_outlier_shifting_values_list.append(shifting_values_1[n])#keep all non-outlier\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[n])\n",
    "        else:\n",
    "            new_outlier_deviation.append(new_deviation[n])\n",
    "    \n",
    "    #if there's no outlier or just one outlier, return final list of scale factor and wavelength \n",
    "    if len(new_outlier_deviation) == 0 or len(new_outlier_deviation) == 1:\n",
    "        return no_outlier_shifting_values_list, no_outlier_wavelength_values_list\n",
    "        exit()\n",
    "    \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(new_outlier_deviation) > 1: \n",
    "        new_outlier_deviation.remove(max(new_outlier_deviation))\n",
    "        for value in new_outlier_deviation:\n",
    "            no_outlier_shifting_values_list.append(shifting_values_1[new_deviation.index(value)])\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[new_deviation.index(value)])\n",
    "        return no_outlier_shifting_values_list,no_outlier_wavelength_values_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_polynomial_fits_shifting(mask_name,slit_num,shifting_values_list,wavelength_values_list):\n",
    "    \n",
    "    no_outlier_shifting_values_list = shifting_values_list #scaling values with outliers removed \n",
    "    no_outlier_wavelength_values_list = wavelength_values_list #wavelength values with outliers removed\n",
    "    \n",
    "    poly_const_second_deg = np.polyfit(no_outlier_wavelength_values_list,no_outlier_shifting_values_list,2) #find the polynomial constants\n",
    "    print(\"New Second Order Polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "    \n",
    "    #calculate the new second order polynomial fits\n",
    "    \n",
    "    poly_second_order = [] \n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "    #straighten the ends\n",
    "        \n",
    "    #Optimal Scaling Factor of far left wavelength\n",
    "    left_end_val = polynomial_second_order(min(no_outlier_wavelength_values_list),poly_const_second_deg) \n",
    "    \n",
    "    #Optimal Scaling Factor of far right wavelength\n",
    "    right_end_val = polynomial_second_order(max(no_outlier_wavelength_values_list),poly_const_second_deg) \n",
    "    \n",
    "    print(\"Optimal Shift Factor of All Wavelength Before {0} Angstroms: {1}\".format(min(no_outlier_wavelength_values_list),left_end_val)) #print them out \n",
    "    \n",
    "    print(\"Optimal Shift Factor of All Wavelength After {0} Angstroms: {1} \".format(max(no_outlier_wavelength_values_list),right_end_val))\n",
    "    \n",
    "    #straighten the LEFT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] < min(no_outlier_wavelength_values_list):\n",
    "            poly_second_order[n] = left_end_val\n",
    "        \n",
    "    #straighten the RIGHT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] > max(no_outlier_wavelength_values_list):\n",
    "            poly_second_order[n] = right_end_val\n",
    "        \n",
    "    #plot\n",
    "    fig,axs = plt.subplots(1)\n",
    "    fig.set_size_inches(8,6)\n",
    "    fig.patch.set_alpha(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.25,0.25)\n",
    "    axs.set_title(\"Mask {0}: Slit #{1}\\nOptimal Shift Factor vs Wavelength\".format(mask_name,slit_num))\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Shift Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.scatter(wavelength_values_list,shifting_values_list,label=\"Optimal Shift Factor\",c=\"black\")\n",
    "    fig.savefig(optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Polynomial_Graph/Shifting_Fitting/Poly_Graph_{0}_slit_{1}.png\".format(mask_name,slit_num))\n",
    "    \n",
    "    return no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_second_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Original Wavelength Using Polynomial Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_new_shifting_factor(wave,no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_second_deg):\n",
    "     \n",
    "    wave = wave #original wavelength from FITS files\n",
    "    \n",
    "    no_outlier_wavelength_values_list = no_outlier_wavelength_values_list #weighted wavelength with outliers removed \n",
    "    \n",
    "    original_wavelength_shift = []\n",
    "    \n",
    "    for n in range(len(wave)): \n",
    "        if wave[n] < min(no_outlier_wavelength_values_list):\n",
    "            original_wavelength_shift.append(wave[n] + left_end_val)\n",
    "        \n",
    "        elif wave[n] > max(no_outlier_wavelength_values_list): \n",
    "            original_wavelength_shift.append(wave[n] + right_end_val)\n",
    "                                      \n",
    "        else:\n",
    "            shift_factor = polynomial_second_order(wave[n],poly_const_second_deg)\n",
    "            original_wavelength_shift.append(wave[n] + shift_factor)\n",
    "            \n",
    "    return original_wavelength_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Shifted Wavelength as FITS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_polynomial_fits_values_shift(no_outlier_scaling_values_list, no_outlier_wavelength_values_list, original_wavelength_shift,mask_name,slit_number_used):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "    \n",
    "    c1 = fits.Column(name='SHIFT_VALUES', array=no_outlier_scaling_values_list, format='E')\n",
    "    c2 = fits.Column(name='WAVELENGTH_VALUES', array=no_outlier_wavelength_values_list, format='E')\n",
    "    c3 = fits.Column(name='ORIGINAL_WAVELENGTH_SHIFTED', array=original_wavelength_shift, format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1, c2, c3]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Shift_Values/Shift_Values_Polynomial_Fits_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Shift Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_shifting_factor(mask_name,slit_number_used,shifting_value_dict):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "    \n",
    "    c1 = fits.Column(name=\"SHIFTING_FACTOR\", array=list(shifting_value_dict.values()), format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1])\n",
    "    \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "    \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Shifting_Factor/Shifting_Factor_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Shifting Polynomial Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shift_polynomial_coeff(mask_name,slit_number_used,poly_const_second_deg_shift,left_end_val,right_end_val,no_outlier_wavelength_values_list):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        \n",
    "    c2 = fits.Column(name='POLYNOMIAL_COEFFICIENTS_SHIFTING', array=poly_const_second_deg_shift, format='E')\n",
    "    c3 = fits.Column(name='END_VALUES', array=[left_end_val,right_end_val], format='E')\n",
    "    c4 = fits.Column(name='NO_OUTLIER_WEIGHTED_WAVE',array=no_outlier_wavelength_values_list,format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c2,c3,c4]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Polynomial_Coefficients/{0}_Shifting_Polynomial_Coefficients/Shift_Poly_Coeff_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Shifting of Included Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@write_to_log('{0}/{1}/{1}_Shifting_Log_Included.log'.format(optimized_data_path,mask_name))\n",
    "def shifting_included_slits(slit_numbers_to_automate):\n",
    "    \n",
    "    #boolean array using median and threshold\n",
    "    median = median_fits\n",
    "    median_boolean_array = median_threshold(median,threshold_median)\n",
    "    \n",
    "    for slit_number in slit_numbers_to_automate:\n",
    "        \n",
    "        slit_number_used = slit_number\n",
    "        print('\\nShifting slit #{0} (Included) from mask {1}.'.format(slit_number_used,mask_name))\n",
    "\n",
    "        index_of_slit = slit_nums.index(slit_number_used) #index of slit \n",
    "        \n",
    "        #Determine the weighted wavelength \n",
    "        weighted_wavelength_list = automating_weighted_wave_multiple_slits(index_of_slit, rbflux_fits, median) #there is some None values. Let's filter it out! \n",
    "        print(\"List of All Weighted Wavelength: {0}\".format(weighted_wavelength_list))\n",
    "\n",
    "        \n",
    "        #Determine the Optimal Shift Factor \n",
    "        print('Finding the Optimal Shift Factor...')\n",
    "        shifting_value_dict,rbflux_shifted_dict = finding_shifting(slit_number_used, flux, wave, ivar,\n",
    "                                                                   median_boolean_array, rbwave_fits[0], index_of_slit)\n",
    "        \n",
    "        #Saving the Shift Factor\n",
    "        saving_shifting_factor(mask_name,slit_number_used,shifting_value_dict)\n",
    "        \n",
    "        #Sort through rbflux_shifted_dict and remove all key-value we don't need (reduce size of saved file)\n",
    "        sorted_rbflux_shifted_dict = sorting_needed_shift_factor(shifting_value_dict,rbflux_shifted_dict)\n",
    "        \n",
    "        #Saving the rebinned flux calculated using the shifted wavelength\n",
    "        saving_rbflux_shifted(mask_name,slit_number_used,sorted_rbflux_shifted_dict)\n",
    "\n",
    "        try:\n",
    "            #Original Polynomial Fits for Shifting\n",
    "            shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting = wavelength_shifting_function(shifting_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Shifting\n",
    "            #Remove Outliers \n",
    "            no_outlier_shifting_values_list, no_outlier_wavelength_values_list_shift = remove_outliers_sigma_clip_shifting(shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting)\n",
    "\n",
    "            #New Polynomial Fits for Shifting\n",
    "            no_outlier_wavelength_values_list_shift,left_end_val_shift,right_end_val_shift,poly_const_second_deg_shift = new_polynomial_fits_shifting(mask_name,slit_number_used,no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift)\n",
    "\n",
    "            #Use New Poly Fits to Shift Original Flux\n",
    "            original_wavelength_shift = use_new_shifting_factor(wave[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_shift,left_end_val_shift,\n",
    "                               right_end_val_shift,poly_const_second_deg_shift)\n",
    "        \n",
    "        except TypeError:\n",
    "            poly_const_second_deg_shift = [0,0,0] #no shifting\n",
    "            no_outlier_shifting_values_list = list(shifting_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_shift = weighted_wavelength_list\n",
    "            original_wavelength_shift = wave[index_of_slit]\n",
    "            \n",
    "        #Saving Shifted Original Wavelength as Fits Files \n",
    "        save_new_polynomial_fits_values_shift(no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift,original_wavelength_shift,mask_name,slit_number_used)\n",
    "        \n",
    "        #Saving Shifting Polynomial Coefficients\n",
    "        save_shift_polynomial_coeff(mask_name,slit_number_used,poly_const_second_deg_shift,left_end_val_shift,right_end_val_shift,no_outlier_wavelength_values_list_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Shifting of Excluded Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@write_to_log('{0}/{1}/{1}_Shifting_Log_Excluded.log'.format(optimized_data_path,mask_name))\n",
    "def shifting_excluded_slits(slit_numbers_to_automate):\n",
    "    \n",
    "    #boolean array using median and threshold\n",
    "    median = median_fits\n",
    "    median_boolean_array = median_threshold(median,threshold_median)\n",
    "    \n",
    "    for slit_number in slit_numbers_to_automate:\n",
    "        \n",
    "        slit_number_used = slit_number\n",
    "        print('\\nShifting slit #{0} (Excluded) from mask {1}.'.format(slit_number_used,mask_name))\n",
    "\n",
    "        index_of_slit = slit_nums_exclude.index(slit_number_used) #index of slit \n",
    "        \n",
    "        #Determine the weighted wavelength \n",
    "        weighted_wavelength_list = automating_weighted_wave_multiple_slits(index_of_slit, rbflux_fits_exclude, median) #there is some None values. Let's filter it out! \n",
    "        print(\"List of All Weighted Wavelength: {0}\".format(weighted_wavelength_list))\n",
    "\n",
    "        \n",
    "        #Determine the Optimal Shift Factor \n",
    "        print('Finding the Optimal Shift Factor...')\n",
    "        shifting_value_dict,rbflux_shifted_dict = finding_shifting(slit_number_used, flux_exclude, wave_exclude,ivar_exclude,\n",
    "                                                                   median_boolean_array, rbwave_fits_exclude[0], index_of_slit)\n",
    "        \n",
    "        #Saving the Shift Factor\n",
    "        saving_shifting_factor(mask_name,slit_number_used,shifting_value_dict)\n",
    "        \n",
    "        #Sort through rbflux_shifted_dict and remove all key-value we don't need (reduce size of saved file)\n",
    "        sorted_rbflux_shifted_dict = sorting_needed_shift_factor(shifting_value_dict,rbflux_shifted_dict)\n",
    "        \n",
    "        #Saving the rebinned flux calculated using the shifted wavelength\n",
    "        saving_rbflux_shifted(mask_name,slit_number_used,sorted_rbflux_shifted_dict)\n",
    "        \n",
    "        try:\n",
    "            #Original Polynomial Fits for Shifting\n",
    "            shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting = wavelength_shifting_function(shifting_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Shifting\n",
    "            #Remove Outliers \n",
    "            no_outlier_shifting_values_list, no_outlier_wavelength_values_list_shift = remove_outliers_sigma_clip_shifting(shifting_values_list,wavelength_values_shift,poly_const_second_deg_shifting)\n",
    "\n",
    "            #New Polynomial Fits for Shifting\n",
    "            no_outlier_wavelength_values_list_shift,left_end_val_shift,right_end_val_shift,poly_const_second_deg_shift = new_polynomial_fits_shifting(mask_name,slit_number_used,no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift)\n",
    "\n",
    "            #Use New Poly Fits to Shift Original Flux\n",
    "            original_wavelength_shift = use_new_shifting_factor(wave_exclude[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_shift,left_end_val_shift,\n",
    "                               right_end_val_shift,poly_const_second_deg_shift)\n",
    "        \n",
    "        except TypeError:\n",
    "            poly_const_second_deg_shift = [0,0,0] #no shifting\n",
    "            no_outlier_shifting_values_list = list(shifting_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_shift = weighted_wavelength_list\n",
    "            original_wavelength_shift = wave_exclude[index_of_slit]\n",
    "            \n",
    "        #Saving Shifted Original Wavelength as Fits Files \n",
    "        save_new_polynomial_fits_values_shift(no_outlier_shifting_values_list,no_outlier_wavelength_values_list_shift,original_wavelength_shift,mask_name,slit_number_used)\n",
    "        \n",
    "        #Saving Shifting Polynomial Coefficients\n",
    "        save_shift_polynomial_coeff(mask_name,slit_number_used,poly_const_second_deg_shift,left_end_val_shift,right_end_val_shift,no_outlier_wavelength_values_list_shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifting - Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slit_numbers_you_want_to_shift_included_slits = slit_nums #put in the slit numbers you want to run \n",
    "\n",
    "# The text output is written to a log file.\n",
    "# The log file is overwritten (not appended) every time this cell is executed.\n",
    "# If you want to keep a record of particular run, make sure you save a copy of the file before running this cell again.\n",
    "\n",
    "shifting_included_slits(slit_numbers_you_want_to_shift_included_slits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slit_numbers_you_want_to_shift_excluded_slits = slit_nums_exclude #put the slit number of all slits you want to iterate\n",
    "\n",
    "# The text output is written to a log file.\n",
    "# The log file is overwritten (not appended) every time this cell is executed.\n",
    "# If you want to keep a record of particular run, make sure you save a copy of the file before running this cell again.\n",
    "    \n",
    "shifting_excluded_slits(slit_numbers_you_want_to_shift_excluded_slits)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_shifted_wavelength(slit_nums,mask_name):\n",
    "    \n",
    "#     shifted_original_wavelength_fits = []\n",
    "    \n",
    "#     for slit_num in slit_nums:\n",
    "        \n",
    "#         shifted_wavelength = fits.open((\"{2}/{0}/{0}_Rebinned/{0}_Shift_Values/Shift_Values_Polynomial_Fits_{0}_{1}.fits.gz\".format(mask_name,slit_num,optimized_data_path)))\n",
    "#         shifted_original_wavelength_fits.append(shifted_wavelength[1].data[\"ORIGINAL_WAVELENGTH_SHIFTED\"])\n",
    "    \n",
    "#     return shifted_original_wavelength_fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geting Back Shift Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_shift_factor(mask_name,slit_number_used):\n",
    "    \n",
    "    shift_factor_hdu = fits.open(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Shifting_Factor/Shifting_Factor_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    shift_factor = shift_factor_hdu[1].data[\"SHIFTING_FACTOR\"]\n",
    "            \n",
    "    return shift_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Back Flux Rebinned With Shifted Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rbflux_shifted_wave(mask_name,slit_number_used):\n",
    "    read_dict = np.load(\"{2}/{0}/{0}_Rebinned/{0}_Rebinned_Flux_Shifted_Wave/rbflux_shifted_dict_{0}_{1}.npy\".format(mask_name,slit_number_used,optimized_data_path),allow_pickle=True).item()\n",
    "    return read_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shifted_flux_array(index_of_slit,rbflux_shifted_dict,rebinned_flux,shifting_factors):\n",
    "    \n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    \n",
    "    new_flux_joined = []\n",
    "\n",
    "    # Rounding shift factors so that they can work with rbflux_shifted_dict\n",
    "    shifting_values_str = [str(round(x,2)) if (x!=0)  else '0.0' for x in shifting_factors]\n",
    "    #shifting_values_str = [str(round(x,2)) if ((x!=0) and (np.isfinite(x)==True)) else '0.0' for x in shifting_factors]\n",
    "    #print(shifting_values_str)\n",
    "    \n",
    "    for index in range(len(wavelength_array)):\n",
    "        \n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "        \n",
    "        else: \n",
    "            \n",
    "            spectrum = rebinned_flux[index_of_slit] #original rebinned flux \n",
    "\n",
    "            #For segments that DOES NOT have Opt Shifting Factor\n",
    "            if np.isfinite(shifting_factors[index]) == False: \n",
    "                fx = np.array(spectrum)\n",
    "                l = list(fx[np.where((new_wave_600>=wavelength_array[index]) & (new_wave_600<wavelength_array[index+1]))])\n",
    "                new_flux_joined = new_flux_joined + l\n",
    "                    \n",
    "                            \n",
    "            #For segments that do have Opt Shifting Factor\n",
    "            else: \n",
    "                spectrum_shifted = rbflux_shifted_dict[\"Shifted_{0}\".format(shifting_values_str[index])]\n",
    "                fx = np.array(spectrum_shifted)\n",
    "                l = list(fx[np.where((new_wave_600>=wavelength_array[index]) & (new_wave_600<wavelength_array[index+1]))])\n",
    "                new_flux_joined = new_flux_joined + l\n",
    "    \n",
    "    return np.array(new_flux_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_shifted_flux(slit_numbers_to_automate, incl_or_excl):\n",
    "    if (incl_or_excl == 'Included'):\n",
    "        fluxes = rbflux_fits\n",
    "        slits = slit_nums\n",
    "    else:\n",
    "        fluxes = rbflux_fits_exclude\n",
    "        slits = slit_nums_exclude\n",
    "    \n",
    "    shifted_fluxes = []\n",
    "    for slit_number_used in slit_numbers_to_automate:\n",
    "#         print(slit_number_used)\n",
    "        \n",
    "        index_of_slit = slits.index(slit_number_used)\n",
    "        \n",
    "        # Read in saved dictionary flux rebinned with shifted wavelength\n",
    "        rbflux_shifted_dict = read_rbflux_shifted_wave(mask_name,slit_number_used)\n",
    "        \n",
    "        # Read in saved shift factors\n",
    "        shift_factors = read_shift_factor(mask_name,slit_number_used)\n",
    "\n",
    "        # Generate flux array rebinned with shifted wavelength\n",
    "        rbflux_shifted = get_shifted_flux_array(index_of_slit,rbflux_shifted_dict,fluxes,shift_factors)\n",
    "        shifted_fluxes.append(rbflux_shifted)\n",
    "    \n",
    "    return (np.array(shifted_fluxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_fluxes_included = looping_shifted_flux(slit_nums, 'Included')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_fluxes_excluded = looping_shifted_flux(slit_nums_exclude, 'Excluded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making The New Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_median = find_median(shifted_fluxes_included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving The New Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportToFitsNewMedian(median,mask_name):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "        \n",
    "    c1 = fits.Column(name='NEW MEDIAN',array=median,format=\"E\")\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1])\n",
    "        \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "        \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_New_Median/New_Median_of_{0}.fits.gz'.format(mask_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exportToFitsNewMedian(new_median,mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Back New Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_med_from_fits(mask_name):\n",
    "    median_read = fits.open(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_New_Median/New_Median_of_{0}.fits.gz'.format(mask_name))\n",
    "    median_fits = median_read[1].data[\"NEW MEDIAN\"] #contain the median \n",
    "    return median_fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_median_fits = get_new_med_from_fits(mask_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbflux_minus_median(rbflux, median, multipliers,\n",
    "                       use_moving_median=use_moving_median):\n",
    "    \n",
    "    subtraction_dict = {}\n",
    "    \n",
    "    if use_moving_median==True:\n",
    "        median_baseline = moving_median(rbflux-median,  window=window)\n",
    "        rbflux = rbflux - median_baseline\n",
    "        for multiplier in multipliers: \n",
    "\n",
    "            subtraction_list = (multiplier * rbflux) - median\n",
    "            subtraction_list = subtraction_list - moving_median(subtraction_list,  window=window)\n",
    "            subtraction_dict[\"Multiplier_{}\".format(round(multiplier,2))] = subtraction_list\n",
    "            \n",
    "    else:\n",
    "        for multiplier in multipliers: \n",
    "\n",
    "            subtraction_list = (multiplier * rbflux) - median\n",
    "            subtraction_dict[\"Multiplier_{}\".format(round(multiplier,2))] = subtraction_list\n",
    "    \n",
    "    return subtraction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_rms(multiply_boolean, subtraction_dict, multipliers): #both inputs are same length\n",
    "    \n",
    "    rms_dict_sorted = {}\n",
    "    \n",
    "    for multiplier in multipliers:\n",
    "                \n",
    "        subtraction = subtraction_dict[\"Multiplier_{}\".format(round(multiplier,2))]\n",
    "            \n",
    "        rms_dict_sorted[\"Multiplier_{}\".format(round(multiplier,2))] = subtraction[np.where(multiply_boolean == True)]\n",
    "                    \n",
    "    return rms_dict_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_calculation(rms_dict_sorted, multipliers):\n",
    "    \n",
    "    try:\n",
    "        rms_dict = {}\n",
    "\n",
    "        for multiplier in multipliers:\n",
    "\n",
    "            values_for_rms_cal = rms_dict_sorted[\"Multiplier_{}\".format(round(multiplier,2))]\n",
    "            \n",
    "            values_for_rms_cal = np.array(values_for_rms_cal)\n",
    "            rms = np.nanmean(values_for_rms_cal*values_for_rms_cal)**0.5\n",
    "#             rms = statistics.stdev(values_for_rms_cal)\n",
    "            \n",
    "            if not (np.isfinite(rms)):\n",
    "                raise ValueError\n",
    "\n",
    "            rms_dict[\"Multiplier_{}\".format(round(multiplier,2))] = rms\n",
    "        \n",
    "        return rms_dict\n",
    "    \n",
    "    except:\n",
    "        print(\"Everything is False. There's no True boolean. Therefore, RMS cannot be calculated.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_the_rms(slit_number,multipliers, rms_dict, min_wave, max_wave):\n",
    "\n",
    "    path = optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Polynomial_Graph/Scaling_vs_RMS/Slit_{1}\".format(mask_name,slit_number)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    try:\n",
    "        value_list = []\n",
    "\n",
    "        for multiplier in multipliers: \n",
    "\n",
    "            value_list.append(rms_dict[\"Multiplier_{0}\".format(round(multiplier,2))])\n",
    "\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        fig.patch.set_alpha(1)\n",
    "        plt.plot(multipliers,value_list)\n",
    "        plt.xlabel(\"Scaling\")\n",
    "        plt.ylabel(\"RMS\")\n",
    "        plt.title(\"Mask {0}: Slit #{1}\\nRMS vs Scaling ({2} A to {3} A)\".format(mask_name,slit_number,min_wave,max_wave))\n",
    "        fig.savefig(path+'/{0}_Slit_{1}_{2}_to_{3}.png'.format(mask_name,slit_number,min_wave,max_wave))\n",
    "        \n",
    "        min_val = min(value_list)\n",
    "\n",
    "        scale = round(multipliers[value_list.index(min_val)],2)\n",
    "\n",
    "        print(\"Scaling w/ minimum RMS (Slit #{0}): {1}\".format(slit_number,scale) + \" ({0} A to {1} A)\".format(min_wave,max_wave))\n",
    "        \n",
    "        return scale\n",
    "        \n",
    "    except:\n",
    "        print(\"Because we have no RMS there is no plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Automating All The Functions Used In Scaling Process (For A Single Slit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_scaling(slit_number, median_and_subtraction_boolean_array,\n",
    "                    wavelength, min_wave, max_wave,\n",
    "                    multipliers, subtraction_dict): \n",
    "    \n",
    "    #all the functions combined together here for convenience!\n",
    "    \n",
    "    #boolean array using wavelength\n",
    "    wavelength_boolean_array = create_wave_bool(wavelength, min_wave, max_wave)\n",
    "    \n",
    "    #multiply the two boolean arrays\n",
    "    multiply_boolean = median_and_subtraction_boolean_array * wavelength_boolean_array\n",
    "    \n",
    "#     subtraction_dict = subtraction_dict\n",
    "    \n",
    "    #sort through all the subtractions and keep only those that are True\n",
    "    rms_dict_sorted = sorting_rms(multiply_boolean, subtraction_dict, multipliers)\n",
    "    \n",
    "    #calculate the RMS associated with each scaling factor \n",
    "    rms_dict = rms_calculation(rms_dict_sorted, multipliers)\n",
    "    \n",
    "    #plot RMS vs scaling factor\n",
    "    scaling_value = plotting_the_rms(slit_number,multipliers,rms_dict,min_wave,max_wave)\n",
    "\n",
    "    return scaling_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def looping_finding_scale(slit_number, subtraction_dict, median_and_subtraction_boolean_array):\n",
    "\n",
    "    #wavelength_array =  np.arange(4000,11200,200)\n",
    "    wavelength_array = np.arange(4000,11500,500)\n",
    "    #wavelength_array = np.arange(4000, 11350, 350)\n",
    "    \n",
    "    scaling_value_dict = {}\n",
    "\n",
    "    for index in range(len(wavelength_array)):\n",
    "\n",
    "        if (index + 1) == len(wavelength_array): \n",
    "            break\n",
    "\n",
    "        else:\n",
    "        \n",
    "            scaling_value = finding_scaling(slit_number, median_and_subtraction_boolean_array,\n",
    "                                            rbwave_fits[0], wavelength_array[index], wavelength_array[index+1],\n",
    "                                            multipliers, subtraction_dict)\n",
    "            \n",
    "            scaling_value_dict[\"{0}_to_{1}\".format(wavelength_array[index],wavelength_array[index+1])] = scaling_value\n",
    "            \n",
    "    return scaling_value_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Fit for Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelength_scaling_function(scaling_value_dict, weighted_wavelength): #used to make a plot of optimal scale factor vs wavelength\n",
    "    \n",
    "    scaling_values = [] #do not plot any scaling value that has None\n",
    "    \n",
    "    wavelength_values = [] #contains all the wavelength we will plots \n",
    "    \n",
    "    for index in range(len(scaling_value_dict.values())): #filtering out the None values\n",
    "        \n",
    "        if list(scaling_value_dict.values())[index] != None:\n",
    "        \n",
    "            scaling_values.append(list(scaling_value_dict.values())[index])\n",
    "            \n",
    "            wavelength_values.append(weighted_wavelength[index])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "\n",
    "    #finding the polynomial constant for second order\n",
    "    poly_const_second_deg = np.polyfit(wavelength_values,scaling_values,2)\n",
    "    print(\"Second order polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    \n",
    "    #finding the polynomial constant for third order\n",
    "    poly_const_third_deg = np.polyfit(wavelength_values,scaling_values,3)\n",
    "    print(\"Third order polynomial: y = {0}(x^3) + {1}(x^2) + {2}(x) + {3}\".format(poly_const_third_deg[0],\n",
    "                                                                                  poly_const_third_deg[1],poly_const_third_deg[2],poly_const_third_deg[3]))\n",
    "    \n",
    "    #finding the polynomial constant for fourth order\n",
    "    poly_const_fourth_deg = np.polyfit(wavelength_values,scaling_values,4)\n",
    "    print(\"Fourth order polynomial: y = {0}(x^4) + {1}(x^3) + {2}(x^2) + {3}(x) + {4}\".format(poly_const_fourth_deg[0],\n",
    "                                                                                  poly_const_fourth_deg[1],poly_const_fourth_deg[2],poly_const_fourth_deg[3]\n",
    "                                                                                             ,poly_const_fourth_deg[4]))\n",
    "    \n",
    "    \n",
    "    #calculating the second, third,and fourth order polynomial as a function of wavelength\n",
    "    poly_second_order = [] #green line\n",
    "    \n",
    "    poly_third_order = [] #blue line\n",
    "    \n",
    "    poly_fourth_order = [] #orange line\n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "        poly_third_order.append(polynomial_third_order(value,poly_const_third_deg))\n",
    "        \n",
    "        poly_fourth_order.append(polynomial_fourth_order(value,poly_const_fourth_deg))\n",
    "                                                                                            \n",
    "    \n",
    "    #plotting the scaling factors, third-order polynomial, fourth-order polynomial as a function of wavelength\n",
    "    fig,axs = plt.subplots(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.5,2.5)\n",
    "    axs.set_title(\"Optimal Scale Factor vs Wavelength\")\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Scale Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.plot(all_wavelength_values,poly_third_order,scalex=False,scaley=False,label=\"Third-Order\")\n",
    "    axs.plot(all_wavelength_values,poly_fourth_order,scalex=False,scaley=False,label=\"Fourth-Order\")\n",
    "    axs.scatter(wavelength_values,scaling_values,label=\"Optimal Scale Factor\",c=\"black\")\n",
    "    axs.legend()\n",
    "    \n",
    "    return scaling_values,wavelength_values,poly_const_second_deg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving The Polynomial Fits (Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_sigma_clip_scaling(scaling_values_list,weighted_wavelength_list,poly_const_second_deg):\n",
    "    \n",
    "    #optimal shifting value - optimal shifting value based on line of best fit\n",
    "    deviation = [] \n",
    "    \n",
    "    #determine the vertical difference (deviation) between dots and line of best fit\n",
    "    for n in range(len(weighted_wavelength_list)):\n",
    "        deviation.append(np.abs(polynomial_second_order(weighted_wavelength_list[n],poly_const_second_deg) - scaling_values_list[n]))\n",
    "        \n",
    "    print(\"First Deviation: {}\".format(deviation))\n",
    "    print(\"First Deviation RMS: {}\".format(statistics.stdev(deviation)))\n",
    "    \n",
    "    \n",
    "    #BEGIN FIRST ITERATION OF SIGMA-CLIPPING\n",
    "    \n",
    "    #variables to store scaling factor and wavelength from first iteration \n",
    "    scaling_values_1 = [] #scaling factor that is not an outlier\n",
    "    wavelength_values_1 = [] #wavelength that is not an outlier\n",
    "    \n",
    "    #variables to store outliers \n",
    "    outlier_deviation = []\n",
    "    \n",
    "    #remove outliers from data\n",
    "    for n in range(len(deviation)):\n",
    "        if deviation[n]/statistics.stdev(deviation) < 3.0:\n",
    "            scaling_values_1.append(scaling_values_list[n])#keep all non-outlier\n",
    "            wavelength_values_1.append(weighted_wavelength_list[n])\n",
    "        #add all outliers to a separate list\n",
    "        else: \n",
    "            outlier_deviation.append(deviation[n])\n",
    "    \n",
    "    #if there's no outlier, return inputs \n",
    "    if len(outlier_deviation) == 0: \n",
    "        return scaling_values_1, wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(outlier_deviation) > 1: \n",
    "        outlier_deviation.remove(max(outlier_deviation))\n",
    "        for value in outlier_deviation:\n",
    "            scaling_values_1.append(scaling_values_list[deviation.index(value)])\n",
    "            wavelength_values_1.append(weighted_wavelength_list[deviation.index(value)])\n",
    "     \n",
    "    #if there is only one values left after the outliers is removed, return scaling values and wavelength\n",
    "    if len(scaling_values_1) == 0 or len(scaling_values_1) == 1:\n",
    "        return scaling_values_1,wavelength_values_1\n",
    "        exit()\n",
    "    \n",
    "    \n",
    "    #PERFORM A NEW FITTING\n",
    "    \n",
    "    #calculate new polynomial coefficients\n",
    "    new_poly_coeff = np.polyfit(wavelength_values_1,scaling_values_1,2)\n",
    "    \n",
    "    #CALCULATE NEW DEVIATION AND REMOVE OUTLIERS\n",
    "    \n",
    "    new_deviation = []\n",
    "    \n",
    "    for n in range(len(wavelength_values_1)):\n",
    "        new_deviation.append(np.abs(polynomial_second_order(wavelength_values_1[n],new_poly_coeff) - scaling_values_1[n]))\n",
    "    \n",
    "    print(\"Second Deviation: {}\".format(new_deviation))\n",
    "    print(\"Second Deviation RMS: {}\".format(statistics.stdev(new_deviation)))\n",
    "                             \n",
    "    #final set of data with outliers removed\n",
    "    no_outlier_scaling_values_list = []\n",
    "    no_outlier_wavelength_values_list = []\n",
    "    \n",
    "    #to stores outlier deviation for second iteration\n",
    "    new_outlier_deviation = []\n",
    "    \n",
    "    #remove outliers\n",
    "    for n in range(len(new_deviation)):\n",
    "        if new_deviation[n]/statistics.stdev(new_deviation) < 3.0:\n",
    "            no_outlier_scaling_values_list.append(scaling_values_1[n])#keep all non-outlier\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[n])\n",
    "        else:\n",
    "            new_outlier_deviation.append(new_deviation[n])\n",
    "\n",
    "    #if there's no outlier, return inputs \n",
    "    if len(new_outlier_deviation) == 0 or len(new_outlier_deviation) == 1: \n",
    "        return no_outlier_scaling_values_list,no_outlier_wavelength_values_list\n",
    "        exit()\n",
    "        \n",
    "    #if there are outliers, remove the largest one and keep the remaining outliers\n",
    "    elif len(new_outlier_deviation) > 1: \n",
    "        new_outlier_deviation.remove(max(new_outlier_deviation))\n",
    "        for value in new_outlier_deviation:\n",
    "            no_outlier_scaling_values_list.append(scaling_values_1[new_deviation.index(value)])\n",
    "            no_outlier_wavelength_values_list.append(wavelength_values_1[new_deviation.index(value)])\n",
    "        return no_outlier_scaling_values_list,no_outlier_wavelength_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_polynomial_fits(mask_name,slit_num,scaling_values_list,wavelength_values_list):\n",
    "    \n",
    "    no_outlier_scaling_values_list = scaling_values_list #scaling values with outliers removed \n",
    "    no_outlier_wavelength_values_list = wavelength_values_list #wavelength values with outliers removed\n",
    "    \n",
    "    poly_const_second_deg = np.polyfit(no_outlier_wavelength_values_list,no_outlier_scaling_values_list,2) #find the polynomial constants\n",
    "    print(\"New Second Order Polynomial: y = {0}x^2 + {1}x + {2}\".format(poly_const_second_deg[0],\n",
    "                                                                    poly_const_second_deg[1],poly_const_second_deg[2]))\n",
    "    \n",
    "    all_wavelength_values = np.arange(4000,11000,0.65) #used to plot every single values between 4000 and 11000 using our polynomial\n",
    "    \n",
    "    #calculate the new second order polynomial fits\n",
    "    \n",
    "    poly_second_order = [] \n",
    "    \n",
    "    for value in all_wavelength_values:\n",
    "        poly_second_order.append(polynomial_second_order(value,poly_const_second_deg))\n",
    "        \n",
    "        \n",
    "        \n",
    "    #straighten the ends\n",
    "        \n",
    "    #Optimal Scaling Factor of far left wavelength\n",
    "    left_end_val = polynomial_second_order(min(no_outlier_wavelength_values_list),poly_const_second_deg) \n",
    "    \n",
    "    #Optimal Scaling Factor of far right wavelength\n",
    "    right_end_val = polynomial_second_order(max(no_outlier_wavelength_values_list),poly_const_second_deg) \n",
    "    \n",
    "    print(\"Optimal Scale Factor of All Wavelength Before {0} Angstroms: {1}\".format(min(no_outlier_wavelength_values_list),left_end_val)) #print them out \n",
    "    \n",
    "    print(\"Optimal Scale Factor of All Wavelength After {0} Angstroms: {1} \".format(max(no_outlier_wavelength_values_list),right_end_val))\n",
    "    \n",
    "    #straighten the LEFT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] < min(no_outlier_wavelength_values_list):\n",
    "            poly_second_order[n] = left_end_val\n",
    "        \n",
    "    #straighten the RIGHT side of the polynomial fit\n",
    "    for n in range(len(all_wavelength_values)):\n",
    "        if all_wavelength_values[n] > max(no_outlier_wavelength_values_list):\n",
    "            poly_second_order[n] = right_end_val\n",
    "    \n",
    "    \n",
    "    #plot\n",
    "    fig,axs = plt.subplots(1)\n",
    "    fig.set_size_inches(8,6)\n",
    "    fig.patch.set_alpha(1)\n",
    "    axs.set_xlim(3900,11100)\n",
    "    axs.set_ylim(-0.5,2.5)\n",
    "    axs.set_title(\"Mask {0}: Slit #{1}\\nOptimal Scale Factor vs Wavelength\".format(mask_name,slit_num))\n",
    "    axs.set_xlabel(\"Wavelength (A)\")\n",
    "    axs.set_ylabel(\"Optimal Scale Factor\")\n",
    "    axs.plot(all_wavelength_values,poly_second_order,scalex=False,scaley=False,label=\"Second-Order\",c=\"green\")\n",
    "    axs.scatter(wavelength_values_list,scaling_values_list,label=\"Optimal Scale Factor\",c=\"black\")\n",
    "    fig.savefig(optimized_data_path + '/{0}'.format(mask_name) + \"/{0}_Polynomial_Graph/Scaling_Fitting/Poly_Graph_{0}_slit_{1}.png\".format(mask_name,slit_num))\n",
    "    \n",
    "    return no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_second_deg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Original Flux Using Polynomial Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_new_scaling_factor(flux_include,wave_include,no_outlier_wavelength_values_list,left_end_val,right_end_val,poly_const_second_deg):\n",
    "    \n",
    "    flux = flux_include #original flux from FITS files \n",
    "    wave = wave_include #original wavelength from FITS files\n",
    "    \n",
    "    no_outlier_wavelength_values_list = no_outlier_wavelength_values_list #weighted wavelength with outliers removed \n",
    "    \n",
    "    original_flux_scale = []\n",
    "    \n",
    "    for n in range(len(wave_include)): \n",
    "        if wave_include[n] < min(no_outlier_wavelength_values_list):\n",
    "            original_flux_scale.append(flux_include[n] * left_end_val)\n",
    "        \n",
    "        elif wave_include[n] > max(no_outlier_wavelength_values_list): \n",
    "            original_flux_scale.append(flux_include[n] * right_end_val)\n",
    "                                      \n",
    "        else:\n",
    "            scale_factor = polynomial_second_order(wave_include[n],poly_const_second_deg)\n",
    "            original_flux_scale.append(flux_include[n] * scale_factor)\n",
    "            \n",
    "    return original_flux_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Scaled Flux as FITS Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_polynomial_fits_values_scale(no_outlier_scaling_values_list, no_outlier_wavelength_values_list, original_flux_scale, mask_name,slit_number_used):\n",
    "            \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "        \n",
    "    c1 = fits.Column(name='SCALE_VALUES', array=no_outlier_scaling_values_list, format='E')\n",
    "    c2 = fits.Column(name='WAVELENGTH_VALUES', array=no_outlier_wavelength_values_list, format='E')\n",
    "    c3 = fits.Column(name='ORIGINAL_FLUX_SCALED', array=original_flux_scale, format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1, c2, c3]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Scale_Values/Scale_Values_Polynomial_Fits_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Scale Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saving_scaling_factor(mask_name,slit_number_used,scaling_value_dict):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU()\n",
    "    \n",
    "    c1 = fits.Column(name=\"SCALING_FACTOR\", array=list(scaling_value_dict.values()), format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1])\n",
    "    \n",
    "    hdul = fits.HDUList([hdu1,hdu2])\n",
    "    \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Scaling_Factor/Scaling_Factor_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Scaling Polynomial Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scale_polynomial_coeff(mask_name,slit_number_used,poly_const_second_deg_scal,left_end_val,right_end_val,no_outlier_wavelength_values_list):\n",
    "    \n",
    "    hdu1 = fits.PrimaryHDU() #primary HDU (empty)\n",
    "    \n",
    "    c1 = fits.Column(name='POLYNOMIAL_COEFFICIENTS_SCALING', array=poly_const_second_deg_scal, format='E')\n",
    "    c3 = fits.Column(name='END_VALUES', array=[left_end_val,right_end_val], format='E')\n",
    "    c4 = fits.Column(name='NO_OUTLIER_WEIGHTED_WAVE',array=no_outlier_wavelength_values_list,format='E')\n",
    "\n",
    "    hdu2 = fits.BinTableHDU.from_columns([c1,c3,c4]) #first extensional HDU (w data)\n",
    "            \n",
    "    hdul = fits.HDUList([hdu1, hdu2]) #combine both HDUs into file and write it below\n",
    "        \n",
    "    hdul.writeto(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Polynomial_Coefficients/{0}_Scaling_Polynomial_Coefficients/Scale_Poly_Coeff_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Scaling of Included Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@write_to_log('{0}/{1}/{1}_Scaling_Log_Included.log'.format(optimized_data_path,mask_name))\n",
    "def scaling_included_slits(slit_numbers_to_automate):\n",
    "    \n",
    "    median = new_median_fits\n",
    "\n",
    "    #boolean array using median and threshold\n",
    "    median_boolean_array = median_threshold(median, threshold_median)\n",
    "    \n",
    "    for slit_number in slit_numbers_to_automate:\n",
    "\n",
    "        slit_number_used = slit_number\n",
    "        print('\\nOptimizing slit #{0} (Included) from mask {1}.'.format(slit_number_used,mask_name))\n",
    "\n",
    "        index_of_slit = slit_nums.index(slit_number_used) #index of slit\n",
    "        \n",
    "        #boolean array using rbflux - median \n",
    "        sky_sub_boolean_array = sky_sub_bool(index_of_slit, shifted_fluxes_included, median, threshold_sky_sub)\n",
    "        \n",
    "        median_and_subtraction_boolean_array = median_boolean_array * sky_sub_boolean_array\n",
    "                \n",
    "        #Determine the Optimal Scale Factor \n",
    "        print('Finding the Optimal Scale Factor...')\n",
    "        subtraction_dict = rbflux_minus_median(shifted_fluxes_included[index_of_slit], median, multipliers)\n",
    "\n",
    "        scaling_value_dict = looping_finding_scale(slit_number_used, subtraction_dict, median_and_subtraction_boolean_array)\n",
    "\n",
    "        #Determine the weighted wavelength \n",
    "        weighted_wavelength_list = automating_weighted_wave_multiple_slits(index_of_slit, shifted_fluxes_included, median) #there is some None values. Let's filter it out! \n",
    "        print(\"List of All Weighted Wavelength: {0}\".format(weighted_wavelength_list))\n",
    "        \n",
    "        try:\n",
    "            #Original Polynomial Fits for Scaling\n",
    "            scaling_values_list,wavelength_values_scal,poly_const_second_deg_scaling = wavelength_scaling_function(scaling_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Scaling\n",
    "            #Remove Outliers Using Sigma Clipping\n",
    "            no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal = remove_outliers_sigma_clip_scaling(scaling_values_list,wavelength_values_scal,poly_const_second_deg_scaling)\n",
    "\n",
    "            #New Polynomial Fits for Scaling\n",
    "            no_outlier_wavelength_values_list_scal,left_end_val_scal,right_end_val_scal,poly_const_second_deg_scal = new_polynomial_fits(mask_name,slit_number_used,no_outlier_scaling_values_list,no_outlier_wavelength_values_list_scal)\n",
    "            print('left end value:',left_end_val_scal)\n",
    "            \n",
    "            #Use New Poly Fits to Scale Original Flux\n",
    "            original_flux_scale = use_new_scaling_factor(shifted_fluxes_included[index_of_slit],wave[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_scal,left_end_val_scal,\n",
    "                               right_end_val_scal,poly_const_second_deg_scal)\n",
    "        except TypeError: \n",
    "            poly_const_second_deg_scal = [0,0,1] #straight horizontal line, no scaling\n",
    "            no_outlier_scaling_values_list = list(scaling_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_scal = weighted_wavelength_list\n",
    "            original_flux_scale = shifted_fluxes_included[index_of_slit]\n",
    "                    \n",
    "        #Saving Scaled Flux as FITS Files\n",
    "        save_new_polynomial_fits_values_scale(no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal, original_flux_scale,mask_name,slit_number_used)\n",
    "\n",
    "        #Saving the Scale Factor\n",
    "        saving_scaling_factor(mask_name,slit_number_used,scaling_value_dict)\n",
    "        \n",
    "        #Saving Scaling Polynomial Coefficients\n",
    "        save_scale_polynomial_coeff(mask_name,slit_number_used,poly_const_second_deg_scal,left_end_val_scal,right_end_val_scal,no_outlier_wavelength_values_list_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Scaling of Excluded Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@write_to_log('{0}/{1}/{1}_Scaling_Log_Excluded.log'.format(optimized_data_path,mask_name))\n",
    "def scaling_excluded_slits(slit_numbers_to_automate):\n",
    "    \n",
    "    median = new_median_fits\n",
    "\n",
    "    #boolean array using median and threshold\n",
    "    median_boolean_array = median_threshold(median, threshold_median)\n",
    "    \n",
    "    for slit_number in slit_numbers_to_automate:\n",
    "\n",
    "        slit_number_used = slit_number\n",
    "        print('\\nOptimizing slit #{0} (Excluded) from mask {1}.'.format(slit_number_used,mask_name))\n",
    "\n",
    "        index_of_slit = slit_nums_exclude.index(slit_number_used) #index of slit\n",
    "\n",
    "        #boolean array using rbflux - median \n",
    "        sky_sub_boolean_array = sky_sub_bool(index_of_slit, shifted_fluxes_excluded, median, threshold_sky_sub)\n",
    "        \n",
    "        median_and_subtraction_boolean_array = median_boolean_array * sky_sub_boolean_array\n",
    "        \n",
    "        #Determine the Optimal Scale Factor \n",
    "        print('Finding the Optimal Scale Factor...')\n",
    "        subtraction_dict = rbflux_minus_median(shifted_fluxes_excluded[index_of_slit], median, multipliers)\n",
    "\n",
    "        scaling_value_dict = looping_finding_scale(slit_number_used, subtraction_dict, median_and_subtraction_boolean_array)\n",
    "\n",
    "        #Determine the weighted wavelength \n",
    "        weighted_wavelength_list = automating_weighted_wave_multiple_slits(index_of_slit, shifted_fluxes_excluded, median) #there is some None values. Let's filter it out! \n",
    "        print(\"List of All Weighted Wavelength: {0}\".format(weighted_wavelength_list))\n",
    "        \n",
    "        try:\n",
    "            #Original Polynomial Fits for Scaling\n",
    "            scaling_values_list,wavelength_values_scal,poly_const_second_deg_scaling = wavelength_scaling_function(scaling_value_dict,weighted_wavelength_list)\n",
    "\n",
    "            #Improving Original Polynomial Fits for Scaling\n",
    "            #Remove Outliers Using Sigma Clipping\n",
    "            no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal = remove_outliers_sigma_clip_scaling(scaling_values_list,wavelength_values_scal,poly_const_second_deg_scaling)\n",
    "\n",
    "            #New Polynomial Fits for Scaling\n",
    "            no_outlier_wavelength_values_list_scal,left_end_val_scal,right_end_val_scal,poly_const_second_deg_scal = new_polynomial_fits(mask_name,slit_number_used,no_outlier_scaling_values_list,no_outlier_wavelength_values_list_scal)\n",
    "\n",
    "            #Use New Poly Fits to Scale Original Flux\n",
    "            original_flux_scale = use_new_scaling_factor(shifted_fluxes_excluded[index_of_slit],wave_exclude[index_of_slit],\n",
    "                               no_outlier_wavelength_values_list_scal,left_end_val_scal,\n",
    "                               right_end_val_scal,poly_const_second_deg_scal)\n",
    "        except TypeError: \n",
    "            poly_const_second_deg_scal = [0,0,1] #straight horizontal line, no scaling\n",
    "            no_outlier_scaling_values_list = list(scaling_value_dict.values())\n",
    "            no_outlier_wavelength_values_list_scal = weighted_wavelength_list\n",
    "            original_flux_scale = shifted_fluxes_excluded[index_of_slit]\n",
    "        \n",
    "        #Saving Scaled Flux as FITS Files\n",
    "        save_new_polynomial_fits_values_scale(no_outlier_scaling_values_list, no_outlier_wavelength_values_list_scal, original_flux_scale,mask_name,slit_number_used)\n",
    "\n",
    "        #Saving the Scale Factor\n",
    "        saving_scaling_factor(mask_name,slit_number_used,scaling_value_dict)\n",
    "        \n",
    "        #Saving Scaling Polynomial Coefficients\n",
    "        save_scale_polynomial_coeff(mask_name,slit_number_used,poly_const_second_deg_scal,left_end_val_scal,right_end_val_scal,no_outlier_wavelength_values_list_scal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling - Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slit_numbers_you_want_to_scale_included_slits = slit_nums #put in the slit numbers you want to run \n",
    "\n",
    "# The text output is written to a log file.\n",
    "# The log file is overwritten (not appended) every time this cell is executed.\n",
    "# If you want to keep a record of particular run, make sure you save a copy of the file before running this cell again.\n",
    "\n",
    "scaling_included_slits(slit_numbers_you_want_to_scale_included_slits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slit_numbers_you_want_to_scale_excluded_slits = slit_nums_exclude #put the slit number of all slits you want to iterate\n",
    "\n",
    "# The text output is written to a log file.\n",
    "# The log file is overwritten (not appended) every time this cell is executed.\n",
    "# If you want to keep a record of particular run, make sure you save a copy of the file before running this cell again.\n",
    "\n",
    "scaling_excluded_slits(slit_numbers_you_want_to_scale_excluded_slits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reading Back Optimized Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Back Shift & Scale Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scale_and_shift_factor(mask_name,slit_number_used):\n",
    "    \n",
    "    scale_factor_hdu = fits.open(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Scaling_Factor/Scaling_Factor_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    scale_factor = scale_factor_hdu[1].data[\"SCALING_FACTOR\"]\n",
    "    \n",
    "    shift_factor = read_shift_factor(mask_name,slit_number_used)\n",
    "            \n",
    "    return scale_factor,shift_factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Back All Polynomial Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_poly_coeff(mask_name,slit_number_used):\n",
    "    \n",
    "    #get the polynomial coefficients and stores them\n",
    "    \n",
    "    shift_poly_coeff = fits.open(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Polynomial_Coefficients/{0}_Shifting_Polynomial_Coefficients/Shift_Poly_Coeff_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    read_poly_coeff_shift = shift_poly_coeff[1].data[\"POLYNOMIAL_COEFFICIENTS_SHIFTING\"]\n",
    "    \n",
    "    scale_poly_coeff = fits.open(optimized_data_path + '/{0}'.format(mask_name) + '/{0}_Rebinned/{0}_Polynomial_Coefficients/{0}_Scaling_Polynomial_Coefficients/Scale_Poly_Coeff_{0}_{1}.fits.gz'.format(mask_name,slit_number_used))\n",
    "    read_poly_coeff_scale = scale_poly_coeff[1].data[\"POLYNOMIAL_COEFFICIENTS_SCALING\"]\n",
    "    end_values = scale_poly_coeff[1].data[\"END_VALUES\"]\n",
    "    no_outlier_weighted_wave = scale_poly_coeff[1].data[\"NO_OUTLIER_WEIGHTED_WAVE\"]\n",
    "    \n",
    "    return read_poly_coeff_scale,read_poly_coeff_shift,end_values,no_outlier_weighted_wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Optimal Shift & Scale Factors (For A Single Slit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizing_using_saved_data(index_of_slit,median,rbflux_shifted_dict,rebinned_flux,shifting_values,poly_coeff_scale,end_values,no_outlier_weighted_wave):\n",
    "   \n",
    "    new_flux_joined = get_shifted_flux_array(index_of_slit,rbflux_shifted_dict,rebinned_flux,shifting_values)\n",
    "    \n",
    "    all_scale_factors = [] #calculate all the scale factor using polynomial function\n",
    "    \n",
    "    for wavelength in new_wave_600:\n",
    "        if wavelength < min(no_outlier_weighted_wave):\n",
    "            all_scale_factors.append(end_values[0])\n",
    "             \n",
    "        elif wavelength > max(no_outlier_weighted_wave):\n",
    "            all_scale_factors.append(end_values[1])\n",
    "        \n",
    "        else:\n",
    "            scaling_factor = polynomial_second_order(wavelength,poly_coeff_scale)\n",
    "            all_scale_factors.append(scaling_factor)\n",
    "    \n",
    "    new_rbflux_scaled = (np.asarray(new_flux_joined) * np.array(all_scale_factors)) - np.array(median)\n",
    "    \n",
    "    return new_rbflux_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation of Included Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automation_func_include(mask_name,slit_number):\n",
    "    \n",
    "    #slit number\n",
    "    slit_number_used = slit_number \n",
    "    \n",
    "    median = new_median_fits\n",
    "    \n",
    "    #index of slit number using list of included slits \n",
    "    index_of_slit = slit_nums.index(slit_number_used)\n",
    "    \n",
    "    #get scale factor and shift factor from FITS files\n",
    "    scale_factor_list,shift_factor_list = read_scale_and_shift_factor(mask_name,slit_number_used)\n",
    "    \n",
    "    #Display whole number, but is actually a float \n",
    "    #Round it to whole number \n",
    "    # rounded_scale_factor_list,rounded_shift_factor_list = rounding_the_factors(scale_factor_list,shift_factor_list)\n",
    "    \n",
    "    #Rebinned flux w/ shifted wavelength \n",
    "    rbflux_shifted_dict = read_rbflux_shifted_wave(mask_name,slit_number_used)\n",
    "    \n",
    "    #Read poly coeff\n",
    "    poly_coeff_scale,poly_coeff_shift,end_values,no_outlier_weighted_wave = read_poly_coeff(mask_name,slit_number)\n",
    "    \n",
    "    #subtraction using new median\n",
    "    new_flux = optimizing_using_saved_data(index_of_slit,median,rbflux_shifted_dict,rbflux_fits,shift_factor_list,poly_coeff_scale,end_values,no_outlier_weighted_wave)    \n",
    "    \n",
    "    return new_flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_all_include_new_flux(mask_name,slit_nums):\n",
    "    new_flux_list = []\n",
    "\n",
    "    for slit_number in slit_nums:\n",
    "\n",
    "        new_flux = automation_func_include(mask_name,slit_number)\n",
    "\n",
    "        new_flux_list.append(new_flux)\n",
    "        \n",
    "    return new_flux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flux_list_include_slits = combining_all_include_new_flux(mask_name,slit_nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation of Excluded Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automation_func_exclude(mask_name,slit_number):\n",
    "    \n",
    "    #slit number\n",
    "    slit_number_used = slit_number \n",
    "    \n",
    "    median = new_median_fits\n",
    "    \n",
    "    #index of slit number using list of excluded slits \n",
    "    index_of_slit = slit_nums_exclude.index(slit_number_used)\n",
    "    \n",
    "    #get scale factor and shift factor from FITS files\n",
    "    scale_factor_list,shift_factor_list = read_scale_and_shift_factor(mask_name,slit_number_used)\n",
    "    \n",
    "    #Display whole number, but is actually a float \n",
    "    #Round it to whole number \n",
    "    # rounded_scale_factor_list,rounded_shift_factor_list = rounding_the_factors(scale_factor_list,shift_factor_list)\n",
    "    \n",
    "    #Rebinned flux w/ shifted wavelength \n",
    "    rbflux_shifted_dict = read_rbflux_shifted_wave(mask_name,slit_number_used)\n",
    "    \n",
    "    #Read poly coeff\n",
    "    poly_coeff_scale,poly_coeff_shift,end_values,no_outlier_weighted_wave = read_poly_coeff(mask_name,slit_number)\n",
    "    \n",
    "    #subtraction using new median\n",
    "    new_flux = optimizing_using_saved_data(index_of_slit,median,rbflux_shifted_dict,rbflux_fits_exclude,shift_factor_list,poly_coeff_scale,end_values,no_outlier_weighted_wave)    \n",
    "    \n",
    "    return new_flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combining_all_exclude_new_flux(mask_name,slit_nums_exclude):\n",
    "    new_flux_list = []\n",
    "\n",
    "    for slit_number in slit_nums_exclude:\n",
    "\n",
    "        new_flux = automation_func_exclude(mask_name,slit_number)\n",
    "\n",
    "        new_flux_list.append(new_flux)\n",
    "        \n",
    "    return new_flux_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flux_list_exclude_slits = combining_all_exclude_new_flux(mask_name,slit_nums_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Optimized Subtraction With Initial Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_subtraction_plots(slits,excl_or_incl):\n",
    "\n",
    "    rbwave = new_wave_600\n",
    "    \n",
    "    if excl_or_incl == 'Excluded':\n",
    "        def opt_flux(mask_name, slit_number):\n",
    "            return (automation_func_exclude(mask_name,slit_number))\n",
    "        rbflux = rbflux_fits_exclude\n",
    "        slit_list = slit_nums_exclude\n",
    "    elif excl_or_incl == 'Included':\n",
    "        def opt_flux(mask_name, slit_number):\n",
    "            return (automation_func_include(mask_name,slit_number))\n",
    "        rbflux = rbflux_fits\n",
    "        slit_list = slit_nums\n",
    "\n",
    "    path = './Compare Subtractions/{0}/{1}/'.format(mask_name,excl_or_incl)\n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError:\n",
    "        if not os.path.isdir(path):\n",
    "            raise\n",
    "    \n",
    "    notes_path = \"./Raja's notes/{0}_notes.txt\".format(mask_name)        \n",
    "    try:\n",
    "        with open(notes_path,'r') as f:\n",
    "            lines = [x.split() for x in f.readlines()]\n",
    "            notes = {int(x[0]) : ' '.join(x[3:]) for x in lines}\n",
    "            print_notes = True\n",
    "    except IOError:\n",
    "        print_notes = False\n",
    "        \n",
    "    def plot_sub_figure(i):\n",
    "        plt.ioff()\n",
    "        \n",
    "        fig = plt.figure(figsize=(16,6),dpi=600)\n",
    "        fig.patch.set_alpha(1)\n",
    "        \n",
    "        y1 = rbflux[slit_list.index(i)] - median_fits\n",
    "        y2 = opt_flux(mask_name,i)\n",
    "        \n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(rbwave,y1,'dodgerblue',ls='-',label='Intial subtraction')\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "        plt.xlabel(\"Angstroms ($\\AA$)\")\n",
    "        plt.title('Intial subtraction', fontsize=14)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(rbwave,y2,'dodgerblue',ls='-',label='Optimized flux')\n",
    "        plt.ylabel(\"Flux (Electron/Hour)\")\n",
    "        plt.xlabel(\"Angstroms ($\\AA$)\")\n",
    "        plt.title('Optimized subtraction', fontsize=14)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.suptitle('Test figure: comparing subtractions\\n slit #{0} in mask {1} ({2})'.format(i,mask_name,excl_or_incl),fontsize=16)\n",
    "        if (print_notes == True):\n",
    "            txt = 'Notes: {0}'.format(notes[i])\n",
    "            plt.figtext(0.95, 0.08, txt, alpha=0.5, wrap=True, ha='right', va='top', fontstyle='italic', fontsize=8)\n",
    "            plt.tight_layout(rect=[0,0.1,0.9,1])\n",
    "        else:\n",
    "            plt.tight_layout(rect=[0,0,1,0.99])\n",
    "        s = 'test_figure_{0}_slit_{1}.png'.format(mask_name,i)\n",
    "        plt.savefig(path+s)\n",
    "        print('Saved figure {0}'.format(s))\n",
    "        plt.close('all')\n",
    "        plt.ion()\n",
    "    \n",
    "    for i in slits:\n",
    "        plot_sub_figure(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_subtraction_plots(slit_nums,\"Included\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_subtraction_plots(slit_nums_exclude,\"Excluded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "# plt.plot(rbwave_fits[0], rbflux_fits_exclude[slit_nums_exclude.index(42)] - median_fits)\n",
    "# lines = np.array([6562.82, 6548.10, 6583.60, 6716.47, 6730.85])\n",
    "# c = 299792\n",
    "# v = -97\n",
    "# lines *= (1+(v/c))\n",
    "# plt.vlines(lines, -999, +999, color='black')\n",
    "# plt.xlim(6700, 6740)\n",
    "# plt.ylim(120, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
