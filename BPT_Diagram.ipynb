{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "from astropy.io import fits \n",
    "from smooth_kevin import smoother\n",
    "import py_specrebin_vec\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rc\n",
    "import py_specrebin\n",
    "import pandas as pd\n",
    "path_name = '.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "**INSTRUCTION**: <br />\n",
    "NOTE:Notebook is separated into three sections. They are separated by the three #BREAK: <br /> \n",
    "    -AGST Subtraction <br />\n",
    "    -BPT Ratios <br />\n",
    "    -Creating BPT Diagram\n",
    "\n",
    "-**AGST (AirGlow and StarLight Contamincation) Subtraction** <br/>\n",
    "1.Create a folder called \"AGST Subtracted Spectra\" (Don't include the quotation mark) in the same space in which you're running this code. Results will be save here.<br />\n",
    "2.Change mask_name to the appropriate mask you're planning on analyzing.<br />\n",
    "3.Below the \"Saving ST and AG subtracted spectra\" section is the first #BREAK. Run everything from there and above.<br />\n",
    "4.Give it a few minutes. The rebinning function will take around 4-5 minutes to run.<br /> \n",
    "5.Resulting airglow and starlight subtracted spectra will be saved as a 2d array formated as such:\n",
    "MaskName_AGST_Subtracted_Spectra.fits.gz <br />\n",
    "6.If you're planning on just getting the AGST spectra, you can continue with a new mask by changing mask_name. If you're planning on getting BPT Ratios continue to next section.\n",
    "\n",
    "-**BPT Ratios** <br/>\n",
    "NOTE: This section only work for M33 mask with 600ZD grating. 1200G doesn't include H-beta and OIII emission lines.<br/>\n",
    "1.Create a folder called \"BPT Diagram CSV\" <br/>\n",
    "2.Below \"Add Redshift to CSV\" section is the second #BREAK. Run everything from there to the first #BREAK. <br/>\n",
    "3.This will find the rebinned flux of H-Alpha, NII, H-Beta, and OIII -> Take the ratios -> Remove any of the NaN and inf ratios, and save the remaining float (decimal numbers) ratios -> Find the redshift and QOP of the remaining ratios using index -> Check to see if a CSV file has been created to save these. If not, create a CSV file and save the mask name, ratios, redshift, and QOP. If yes, open the existing CSV file and add to it. <br/> \n",
    "4.Do this for all mask before proceeding to the third section.\n",
    "\n",
    "-**Creating BPT Diagram** <br/>\n",
    "NOTE: Do not run this section unless you have run section 1 and 2 for all the masks you wish to include. This section will utilize the CSV file you created in section 2.<br/>\n",
    "1.Run just the \"Create BPT Diagram\" section.<br/>\n",
    "2.This will produce and save two diagrams. Each titled \"BPT Diagram\" and \"BPT_Diagram (QOP>=2)\". These are BPT diagram, one with QOP of 3 and one with QOP of 2 and 3, respectively.<br/> \n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Issues**: <br/>\n",
    "Mask D1M33P: ratio_pairs,index_of_pairs have different length. Can't add to CSV. Fix by changing... <br/>\n",
    "index_of_pairs = np.arange(len(slit_nums))[np.isin(pairing,np.array(ratio_pairs))[:,1]] <br/>\n",
    "from 0 to 1\n",
    "\n",
    "pTN1bspec1d slit #171 causing issues. <br/>\n",
    "VerifyError: Unparsable card (CD1_1), fix it first with .verify('fix') <br/>\n",
    "Removed slit from process. Not sure how to fix error. <br/>\n",
    "With slit removed. New error arise when rebinning.<br/>\n",
    "IndexError: index 0 is out of bounds for axis 0 with size 0<br/>\n",
    "Skip mask. \n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Pannel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_name = \"pTS3spec1d\"\n",
    "grating = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Wavelength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_wave_600 = np.arange(4000, 11000, .65) \n",
    "new_wave_1200 = np.arange(6000, 11000, .33) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function To Get Data From FITS File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the original streams for stdout and stderr. To be used for logging output later\n",
    "import sys\n",
    "std_out = sys.stdout; std_err = sys.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_data(file_names,mask_name):\n",
    "    \n",
    "    tot_flux = []\n",
    "    tot_wave = []\n",
    "    tot_ivar = []\n",
    "    \n",
    "    for j in range(len(file_names)):\n",
    "        #read in star data\n",
    "        h_star = fits.open(path_name + '/' + 'data/{0}'.format(mask_name) + '/' + file_names[j], ignore_missing_end = True)\n",
    "        \n",
    "        data_star1 = h_star[1].data\n",
    "        star_flux1 = data_star1['SKYSPEC'][0]\n",
    "        star_wave1 = data_star1['LAMBDA'][0]\n",
    "        star_ivar1 = data_star1['IVAR'][0]\n",
    "        \n",
    "        data_star2 = h_star[2].data\n",
    "        star_flux2 = data_star2['SKYSPEC'][0]\n",
    "        star_wave2 = data_star2['LAMBDA'][0]\n",
    "        star_ivar2 = data_star2['IVAR'][0]\n",
    "        \n",
    "        \n",
    "        #combine the blue and red side into one list\n",
    "        star_flux = np.array(list(star_flux1) + list(star_flux2))\n",
    "        star_wave = np.array(list(star_wave1) + list(star_wave2))\n",
    "        star_ivar = np.array(list(star_ivar1) + list(star_ivar2))\n",
    "        \n",
    "        if (sum(star_flux) == 0 and sum(star_ivar) == 0 and sum(star_wave) == 0): #section added so that any slit w/o data will use serendip data instead\n",
    "            try: #some slit do not have data BUT also have no serendip. In that case, just add empty data\n",
    "                file_name_split = file_names[j].split(\".\")\n",
    "                serendip_file_name = \"{0}.{1}.{2}.serendip1.{3}.{4}\".format(file_name_split[0],file_name_split[1],\n",
    "                                                                       file_name_split[2],file_name_split[4],file_name_split[5])\n",
    "                path_to_serendip = fits.open(path_name + '/' + \"data/{0}/{1}\".format(mask_name,serendip_file_name))\n",
    "\n",
    "                star_flux1_serendip = path_to_serendip[1].data[\"SKYSPEC\"][0]\n",
    "                star_flux2_serendip = path_to_serendip[2].data[\"SKYSPEC\"][0]\n",
    "                star_flux_serendip = np.concatenate((star_flux1_serendip,star_flux2_serendip))\n",
    "\n",
    "                star_ivar1_serendip = path_to_serendip[1].data[\"IVAR\"][0]\n",
    "                star_ivar2_serendip = path_to_serendip[2].data[\"IVAR\"][0]\n",
    "                star_ivar_serendip = np.concatenate((star_ivar1_serendip,star_ivar2_serendip))\n",
    "\n",
    "                star_wave1_serendip = path_to_serendip[1].data[\"LAMBDA\"][0]\n",
    "                star_wave2_serendip = path_to_serendip[2].data[\"LAMBDA\"][0]\n",
    "                star_wave_serendip = np.concatenate((star_wave1_serendip,star_wave2_serendip))\n",
    "\n",
    "                tot_flux.append(star_flux_serendip)\n",
    "                tot_wave.append(star_wave_serendip)\n",
    "                tot_ivar.append(star_ivar_serendip)\n",
    "\n",
    "                h_star.close()\n",
    "                \n",
    "            except: #For error about serendip file not existing\n",
    "                #add to above lists\n",
    "                tot_flux.append(star_flux)\n",
    "                tot_wave.append(star_wave)\n",
    "                tot_ivar.append(star_ivar)\n",
    "\n",
    "                h_star.close()\n",
    "        \n",
    "        else:\n",
    "            #add to above lists\n",
    "            tot_flux.append(star_flux)\n",
    "            tot_wave.append(star_wave)\n",
    "            tot_ivar.append(star_ivar)\n",
    "\n",
    "            h_star.close()\n",
    "        \n",
    "    return tot_flux, tot_wave, tot_ivar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function For Rebinning w/ Updated Rebinning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin(fluxes, waves, ivar, grating):\n",
    "    \n",
    "    rbflux = []\n",
    "    rbivar = []\n",
    "    \n",
    "    if grating == 600:\n",
    "        new_wave = new_wave_600\n",
    "    elif grating == 1200:\n",
    "        new_wave = new_wave_1200\n",
    "    \n",
    "    for i in range(len(waves)):\n",
    "        new_flux,new_ivar = py_specrebin_vec.rebinspec(waves[i],fluxes[i],new_wave,ivar=ivar[i])\n",
    "        new_flux_err = 1/np.sqrt(new_ivar)\n",
    "\n",
    "        rbflux.append(new_flux)\n",
    "        rbivar.append(new_ivar)\n",
    "        \n",
    "    return rbflux, new_wave, rbivar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function To Create Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_median(rebinned_flux_array):\n",
    "    \n",
    "    median_vals = []\n",
    "    \n",
    "    for i in range(len(rebinned_flux_array[0])):\n",
    "\n",
    "        comp = []\n",
    "        \n",
    "        for array in rebinned_flux_array:\n",
    "            \n",
    "            if np.isfinite(array[i]) == True:\n",
    "                comp.append(array[i])\n",
    "                \n",
    "        median_vals.append(np.median(comp))\n",
    "        \n",
    "    return median_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function To Read ISM_EM_LINES.txt & Extract Slit # of Excluded Slits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exclusions():\n",
    "    filepath = 'ISM_EM_LINES.txt'\n",
    "    fp = open(filepath)\n",
    "    all_data = []\n",
    "    for line in (fp):\n",
    "        mask_name = line.split(':')[0].split('_')[0]\n",
    "        slit_number = line.split(':')[1].strip().split(\" \")[0]\n",
    "        if len(slit_number) == 2:\n",
    "            slit_number = '0' + slit_number\n",
    "        elif len(slit_number) == 1:\n",
    "            slit_number = '00' + slit_number\n",
    "        else:\n",
    "            pass\n",
    "        object_id = line.split(':')[1].strip().split()[1]\n",
    "        data = {}\n",
    "        data['mask_name'] = mask_name\n",
    "        data['slit_number'] = slit_number\n",
    "        data['object_id'] = object_id\n",
    "        all_data.append(data)\n",
    "    return all_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_to_include(folder):\n",
    "    import os\n",
    "    list_of_files_to_include = []\n",
    "    list_of_files_to_exclude = []\n",
    "    serendip_files = []\n",
    "    all_file_names_in_folder = os.listdir('data/{}'.format(folder))\n",
    "    y = len(all_file_names_in_folder)\n",
    "    print(\"The number of files in the folder is {0}\".format(y))\n",
    "    all_data = get_exclusions()\n",
    "    len_all_data = len(all_data)\n",
    "    for n in range(y):\n",
    "        parts_of_file_name = all_file_names_in_folder[n].split(\".\")\n",
    "        if parts_of_file_name[0] == 'spec1d': # avoids hidden DS_Store files on my mac\n",
    "            object_id = parts_of_file_name[3]\n",
    "            slit_number = parts_of_file_name[2]\n",
    "            mask_name = parts_of_file_name[1]\n",
    "            should_include = True\n",
    "            should_exclude = True\n",
    "            for k in range(len_all_data):\n",
    "                if ((object_id == all_data[k]['object_id']) and (slit_number == all_data[k]['slit_number']) and (mask_name == all_data[k]['mask_name'])):\n",
    "                    should_include = False\n",
    "                    should_exclude = True\n",
    "                if 'serendip' in object_id:\n",
    "                    should_include = False\n",
    "                    should_exclude = False\n",
    "            if should_include == True:\n",
    "                list_of_files_to_include.append(all_file_names_in_folder[n])       \n",
    "            elif should_exclude == True:\n",
    "                list_of_files_to_exclude.append(all_file_names_in_folder[n])\n",
    "            elif should_include == False & should_exclude == False:\n",
    "                serendip_files.append(all_file_names_in_folder[n])\n",
    "    \n",
    "    print('The number of files left after exclusions is {0}'.format(len(list_of_files_to_include)))\n",
    "    \n",
    "    return sorted(list_of_files_to_include), sorted(list_of_files_to_exclude), sorted(serendip_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_slit_nums(files):\n",
    "    \n",
    "    slit_nums = []\n",
    "    \n",
    "    if len(files) > 1:\n",
    "    \n",
    "        for i in range(len(files)):\n",
    "            parts_of_file_name = files[i].split(\".\")\n",
    "            slit_num = parts_of_file_name[2]\n",
    "            slit_nums.append(int(slit_num))\n",
    "            \n",
    "    return slit_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calls To Get Slit Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering files\n",
    "list_of_files_to_include, list_of_files_to_exclude, list_of_serendip_files = get_files_to_include(mask_name)\n",
    "\n",
    "file_names = list_of_files_to_include\n",
    "file_names_exclude = list_of_files_to_exclude\n",
    "file_names_serendip = list_of_serendip_files\n",
    "file_names_all = list_of_files_to_include + list_of_files_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slit_nums = get_slit_nums(file_names) #get slit # of INCLUDED slits\n",
    "slit_nums_exclude = get_slit_nums(file_names_exclude) #get slit # of EXCLUDED slits\n",
    "all_slit_nums = get_slit_nums(file_names_all) #slit # of INCLUDED & EXCLUDED slits\n",
    "\n",
    "print(\"Slit # to INCLUDE in median calculation: {0}\".format(slit_nums))\n",
    "print(\"Slit # to EXCLUDE: {0}\".format(slit_nums_exclude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calls For Data Extraction & Rebinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data\n",
    "#try getting and rebinning all files\n",
    "flux, wave, ivar = get_original_data(file_names, mask_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all excluded data\n",
    "flux_exclude, wave_exclude, ivar_exclude = get_original_data(file_names_exclude, mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the original data\n",
    "rbflux, rbwave, rbivar = rebin(flux, wave, ivar, grating) # this takes about 4 minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rebinning the excluded data\n",
    "rbflux_exclude, rbwave_exclude, rbivar_exclude = rebin(flux_exclude, wave_exclude, ivar_exclude, grating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call For Median Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = find_median(rbflux) #taking the median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import median_filter\n",
    "\n",
    "def moving_median(a, size=325):\n",
    "    \n",
    "    '''\n",
    "    Returns the moving median values of the array,\n",
    "    using a window of a given size, centered at\n",
    "    each point.\n",
    "    \n",
    "    Version - 4.0\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : ndarray\n",
    "        One dimensional flux array.\n",
    "    window : int, optional\n",
    "        The size of each segment for taking the median.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    median_arr : One dimensional array of moving median.\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    all_indices = np.arange(len(a))\n",
    "    finite_bool = np.isfinite(a)\n",
    "    nan_indices = all_indices[np.invert(finite_bool)]\n",
    "    nan_indices_set = set(nan_indices)\n",
    "    n = len(finite_bool)\n",
    "\n",
    "    if (nan_indices_set=={0,n} or nan_indices_set=={0} or nan_indices_set=={n}):\n",
    "        \n",
    "        finite_indices = all_indices[finite_bool]\n",
    "        nearest_finite_indices = np.searchsorted(finite_indices, nan_indices)\n",
    "        nearest_finite_indices = nearest_finite_indices - (nearest_finite_indices==len(finite_indices))\n",
    "        a[nan_indices] = a[finite_indices[nearest_finite_indices]][:]\n",
    "        median_arr = median_filter(a, size, mode='nearest')\n",
    "\n",
    "    elif (len(nan_indices_set)==0):\n",
    "        \n",
    "        median_arr = np.nan*np.ones(len(a))\n",
    "\n",
    "    else:\n",
    "        \n",
    "        if True not in finite_bool:\n",
    "            median_arr = np.nan*np.ones(len(a))\n",
    "            \n",
    "        else:\n",
    "            finite_indices = all_indices[finite_bool]\n",
    "            nearest_finite_indices = np.searchsorted(finite_indices, nan_indices)\n",
    "            gap_indices = ((nearest_finite_indices>0) & (nearest_finite_indices<len(finite_indices)))\n",
    "            middle_nan_indices = nan_indices[gap_indices]\n",
    "            right_nearest_indices = finite_indices[nearest_finite_indices[gap_indices]]\n",
    "            left_nearest_indices = finite_indices[nearest_finite_indices[gap_indices] - 1]\n",
    "            right_distances = abs(right_nearest_indices - middle_nan_indices)\n",
    "            left_distances = abs(left_nearest_indices - middle_nan_indices)\n",
    "            right_is_near_bool = (left_distances > right_distances)\n",
    "            left_is_near_bool = (left_distances <= right_distances)\n",
    "            a[middle_nan_indices[right_is_near_bool]] = a[right_nearest_indices[right_is_near_bool]][:]\n",
    "            a[middle_nan_indices[left_is_near_bool]] = a[left_nearest_indices[left_is_near_bool]][:]\n",
    "            a[nan_indices[nearest_finite_indices==0]] = a[finite_indices[0]]\n",
    "            a[nan_indices[nearest_finite_indices==len(finite_indices)]] = a[finite_indices[-1]]\n",
    "            median_arr = median_filter(a, size, mode='nearest')\n",
    "    \n",
    "    return (median_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Airglow and Starlight Contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGST_subtraction(mask_name,slit_nums,rebinned_flux,median): #airglow and starlight subtraction\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mask_name : str, required\n",
    "        Name of mask.\n",
    "    slit_nums : list, required\n",
    "        List of slit number. \n",
    "    rebinned_flux : list, required\n",
    "        A list containing arrays of rebinned flux. \n",
    "    median : list, required\n",
    "        Calculated median (airglow). \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    all_AGSTsub_spec : list containing 1-d array of airglow and starlight subtracted spectra\n",
    "        \n",
    "    '''\n",
    "        \n",
    "    all_AGSTsub_spec = [] #contains all spectra after airglow and starlight removal \n",
    "        \n",
    "    for slit in slit_nums:\n",
    "        \n",
    "        skysub_spectrum = rebinned_flux[slit_nums.index(slit)] - np.array(median) #airglow subtraction\n",
    "        ST_con = moving_median(skysub_spectrum) #find starlight contamination\n",
    "        AGSTsub_spectrum = skysub_spectrum - ST_con #remove airglow and starlight\n",
    "        all_AGSTsub_spec.append(AGSTsub_spectrum)\n",
    "        \n",
    "    return all_AGSTsub_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGSTsub_spec_incl = AGST_subtraction(mask_name,slit_nums,rbflux,median) #saving 'included' sloits as FITS files\n",
    "AGSTsub_spec_excl = AGST_subtraction(mask_name,slit_nums_exclude,rbflux_exclude,median) #saving 'excluded' slits as FITS files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving ST and AG subtracted spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AGSTsub_spec_to_FITS(mask_name,AGSTsub_spec_incl,AGSTsub_spec_excl):\n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ---\n",
    "    mask_name : str, required\n",
    "        Name of mask.\n",
    "    AGSTsub_spec_incl: list, required\n",
    "        INCLUDED spectra w/ airglow and starlight contamination removed.\n",
    "    AGSTsub_spec_excl: list, required\n",
    "        EXCLUDED spectra w/ airglow and starlight contamination removed.\n",
    "    '''\n",
    "    \n",
    "    hdu0 = fits.PrimaryHDU()\n",
    "    hdu1 = fits.ImageHDU(data=AGSTsub_spec_excl,name=\"RBFLUX EXCLUDE\")\n",
    "    hdu2 = fits.ImageHDU(data=AGSTsub_spec_incl,name=\"RBFLUX INCLUDE\")\n",
    "    hdul = fits.HDUList([hdu0,hdu1,hdu2])\n",
    "    hdul.writeto(\"./AGST Subtracted Spectra/{0}_AGST_Subtracted_Spectra.fits.gz\".format(mask_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGSTsub_spec_to_FITS(mask_name,AGSTsub_spec_incl,AGSTsub_spec_excl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Ratios For BPT Diagram (600 ZD ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPT_ratios(AGSTsub_spec,slit_nums): \n",
    "    \n",
    "    '''\n",
    "    Parameters\n",
    "    ---\n",
    "    AGSTsub_spec: list, req\n",
    "        List containing all EXCLUDED airglow and starlight cont subtracted spectra. \n",
    "        \n",
    "    Returns\n",
    "    ---\n",
    "    ratio_pairs: 2d-array containing pairs of ratio. Format: [NII/HAlpha,OIII/HBeta]\n",
    "    '''\n",
    "    \n",
    "    H_Beta = np.array(AGSTsub_spec)[:,1325] #list containing all H Beta values\n",
    "    OIII = np.array(AGSTsub_spec)[:,1549] #list containing all OIII values\n",
    "    H_Alpha = np.array(AGSTsub_spec)[:,3943] #list containing all H Alpha values\n",
    "    NII = np.array(AGSTsub_spec)[:,3975] #list containing all NII values\n",
    "    \n",
    "    NII_HA_ratios = NII/H_Alpha #1d array containing [NII]6584/Ha ratios\n",
    "    OIII_HB_ratios = OIII/H_Beta #1d array containing [OIII]5007/Hb ratios\n",
    "    \n",
    "    pairing = np.vstack((NII_HA_ratios,OIII_HB_ratios)).T #create pairs:[NII/HAlpha,OIII/HBeta]\n",
    "    \n",
    "    ratio_pairs = []\n",
    "    for element in pairing: #remove any nan (0/0) or inf (float/0). Not all slits have H_Beta or OIII\n",
    "        if (any(np.isinf(element)) == True or any(np.isnan(element)) == True): \n",
    "            pass\n",
    "        else:\n",
    "            ratio_pairs.append(element)\n",
    "    \n",
    "    index_of_pairs = np.arange(len(slit_nums))[np.isin(pairing,np.array(ratio_pairs))[:,1]] #get the index of ratio pairs\n",
    "    \n",
    "    return np.array(ratio_pairs),index_of_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_pairs,index_of_pairs = BPT_ratios(AGSTsub_spec_excl,slit_nums_exclude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Redshift, QOP, and Create DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def loadMarzResults(filepath):\n",
    "    return numpy.genfromtxt(filepath, delimiter=',', skip_header=2, autostrip=True, names=True, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrays containing all slits processed in MARZ\n",
    "res = loadMarzResults(\"./Marz_Results (Redone)/{0}_Marz_KN.mz\".format(mask_name)) \n",
    "confident = res[res['QOP'] == 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = [] #empty list for redshift value\n",
    "QOP = [] #empty list for QOP\n",
    "\n",
    "for n in range(len(res)): #sorting \n",
    "    redshift.append(res[n][12])\n",
    "    QOP.append(res[n][13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Redshift to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Mask Name\":np.full(len(np.array(redshift)[index_of_pairs]),mask_name), #add mask name to DF\n",
    "                   \"[OIII]/H_Beta\":ratio_pairs[:,1], #add [OIII]/H_Beta to DF\n",
    "                   \"[NII]/H_Alpha]\":ratio_pairs[:,0], #add [NII]/H_Alpha] to DF\n",
    "                   \"Redshift\":np.array(redshift)[index_of_pairs], #add redshift to DF\n",
    "                   \"QOP\":np.array(QOP)[index_of_pairs]}) #add QOP to DF\n",
    "\n",
    "if os.path.isfile(\"./BPT Diagram CSV/BPT_Ratios.csv\") == False: #check if file exist. If not make it.\n",
    "    df.to_csv(\"./BPT Diagram CSV/BPT_Ratios.csv\",index=False) \n",
    "else: #if file does exist, open and add to it.\n",
    "    redshift_read = pd.read_csv(\"./BPT Diagram CSV/BPT_Ratios.csv\")\n",
    "    new_csv = pd.concat([redshift_read,df])\n",
    "    new_csv.to_csv(\"./BPT Diagram CSV/BPT_Ratios.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BREAK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BPT Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"./BPT Diagram CSV/BPT_Ratios.csv\") #read csv\n",
    "QOP_3 = df_all[\"QOP\"] == 3\n",
    "OIII_HB_All = np.log10(np.array(df_all[\"[OIII]/H_Beta\"]))[QOP_3] #contain log10 of all values of OIII/HB\n",
    "NII_HA_All = np.log10(np.array(df_all[\"[NII]/H_Alpha]\"]))[QOP_3] #contain log10 of all values of NII/HA\n",
    "fig,ax = plt.subplots(1)\n",
    "plt.scatter(NII_HA_All,OIII_HB_All,s=10)\n",
    "ax.set_title(\"BPT Diagram\")\n",
    "ax.set_xlabel(\"$Log_{10}([NII]/Hα)$\")\n",
    "ax.set_ylabel(\"$Log_{10}([OIII]/Hβ)$\")\n",
    "#fig.set_figwidth(9)\n",
    "#fig.set_figheight(9)\n",
    "plt.savefig(\"BPT_Diagram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(\"./BPT Diagram CSV/BPT_Ratios.csv\") #read csv\n",
    "#QOP_3 = df_all[\"QOP\"] == 3\n",
    "OIII_HB_All = np.log10(np.array(df_all[\"[OIII]/H_Beta\"])) #contain log10 of all values of OIII/HB\n",
    "NII_HA_All = np.log10(np.array(df_all[\"[NII]/H_Alpha]\"])) #contain log10 of all values of NII/HA\n",
    "fig,ax = plt.subplots(1)\n",
    "plt.scatter(NII_HA_All,OIII_HB_All,s=10)\n",
    "ax.set_title(\"BPT Diagram (QOP >= 2)\")\n",
    "ax.set_xlabel(\"$Log_{10}([NII]/Hα)$\")\n",
    "ax.set_ylabel(\"$Log_{10}([OIII]/Hβ)$\")\n",
    "#fig.set_figwidth(10)\n",
    "#fig.set_figheight(10)\n",
    "plt.savefig(\"BPT_Diagram (QOP>=2).png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
